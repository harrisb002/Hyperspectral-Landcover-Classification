{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrisb002/Hyperspectral-Landcover-Classification/blob/main/CombinedCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJx0I4bnoMZz"
      },
      "source": [
        "# (Used for both 1D & 2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrRTgF60ysw4"
      },
      "source": [
        "### Imports, Filepaths, & Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yga9Qqb3ysw5",
        "outputId": "ca6783fc-4285-4e0e-bf9b-c0362608bb3d"
      },
      "outputs": [],
      "source": [
        "!pip install gdal\n",
        "!pip install rasterio\n",
        "!pip install raster2xyz\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install tensorflow\n",
        "!pip install tabulate\n",
        "!pip install scikit-image\n",
        "!pip install openpyxl\n",
        "!pip install tifffile\n",
        "!pip install imagecodecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "74FmWz8yysw6"
      },
      "outputs": [],
      "source": [
        "# File and directory handling\n",
        "import os\n",
        "import shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# Data handling and processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import ast\n",
        "from scipy import stats\n",
        "from scipy.ndimage import zoom, generic_filter\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import (\n",
        "    dilation, square, opening, closing, area_opening,\n",
        "    area_closing, remove_small_objects, remove_small_holes\n",
        ")\n",
        "\n",
        "# Image processing\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from raster2xyz.raster2xyz import Raster2xyz\n",
        "from tifffile import imread\n",
        "import cv2\n",
        "\n",
        "# Data preprocessing and metrics\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, RobustScaler, LabelEncoder,\n",
        "    Normalizer\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, KFold, StratifiedKFold,\n",
        "    StratifiedShuffleSplit, GridSearchCV\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, confusion_matrix,\n",
        "    classification_report, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Conv1D, Flatten, MaxPooling1D,\n",
        "    AveragePooling2D, GlobalAveragePooling1D, Dropout,\n",
        "    BatchNormalization, RandomFlip, Activation\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.patches as mpatches\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Enable inline plots for Jupyter notebooks\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvOfN5xaysw6",
        "outputId": "dd565d5f-707a-4f5b-d00e-f18c8630be2b"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "############## Update your 'mainDir' if your utlizing my code!!! ###############\n",
        "mainDir = \"/content/drive/My Drive/.School/Capstone/\"\n",
        "checkpointDir = \"/content/drive/My Drive/.School/Capstone/Checkpoints/\"\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "# Using Shared Directory\n",
        "# Be careful not to override others Checkpoints! => Make unique to your name\n",
        "################################################################################\n",
        "# Example:\n",
        "# checkpoint_path = mainDir + 'model_checkpoints_{Your_Name_Here}/my_weights.weights.h5'\n",
        "# mainDir = \"/content/drive/Shareddrives/Land_Classification_Training_shared/Land_Classification_training_work/\"\n",
        "################################################################################\n",
        "\n",
        "# Define file paths\n",
        "images_path = mainDir + \"valid_samples_1d/avirisng_samples_combined\"\n",
        "samples_path = mainDir + \"valid_samples_1d/samples.csv\"\n",
        "\n",
        "# DEFINE YOUR MODEL CHECKPOINT\n",
        "# MODEL CHECKPOINTS, ONLY USE TESTING TRAINING, OTHERWISE USE LOADING\n",
        "\n",
        "# 1D Checkpoints\n",
        "################################################################################\n",
        "checkpoint_path = checkpointDir + 'model_checkpoints_Benz_Testing/my_weights.weights.h5'\n",
        "best_checkpoint_path = checkpointDir + 'model_checkpoints_Benz/my_weights.weights.h5'\n",
        "checkpoint_path_class_weights = checkpointDir + 'model_checkpoints_Benz/my_weights_with_cw.weights.h5'\n",
        "checkpoint_path_class_weights_oversampling = checkpointDir + 'model_checkpoints_Benz/my_weights_with_cw_oversampling.weights.h5'\n",
        "################################################################################\n",
        "\n",
        "# 2D Checkpoints\n",
        "################################################################################\n",
        "microcnn_checkpoint_path = checkpointDir + 'microcnn_best.weights.h5'\n",
        "macrocnn_checkpoint_path = checkpointDir + 'macrocnn_best.weights.h5'\n",
        "rgbcnn_checkpoint_path = checkpointDir + 'rgbcnn_best.weights.h5'\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7KyCQKzsROi"
      },
      "source": [
        "### Data Loading, Preprocessing, Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "VH08jDU4srPM"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(samples_df):\n",
        "  \"\"\"\n",
        "  Preprocess the samples DataFrame:\n",
        "  1. Extract Sample_num\n",
        "  2. Clean Labels\n",
        "  3. Remove rows with 'Mixed or Not Classified'\n",
        "  4. Replace NaNs and invalid values in frequency columns\n",
        "  5. Label-encode the labels\n",
        "  \"\"\"\n",
        "  samples_df['Sample_num'] = samples_df['File'].str.split('_').str[0].astype(int)\n",
        "  samples_df['Label'] = samples_df['Label'].str.split('(').str[0].str.strip()\n",
        "\n",
        "  # Remove \"Mixed or Not Classified\"\n",
        "  samples_df = samples_df[samples_df['Label'] != 'Mixed or Not Classified']\n",
        "  samples_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Handle frequency columns\n",
        "  frequency_columns = [col for col in samples_df.columns if col.startswith('frq')]\n",
        "  samples_df.loc[:, frequency_columns] = samples_df[frequency_columns].fillna(-9999)\n",
        "  samples_df = samples_df[~samples_df[frequency_columns].eq(-9999).any(axis=1)]\n",
        "\n",
        "  # Verify no invalid values remain\n",
        "  nan_counts = samples_df[frequency_columns].isna().sum().sum()\n",
        "  assert nan_counts == 0, \"NaN values still exist in frequency columns after replacement\"\n",
        "  count_negative_9999 = (samples_df[frequency_columns] == -9999).sum().sum()\n",
        "  assert count_negative_9999 == 0, \"Rows with -9999 values remain after filtering\"\n",
        "\n",
        "  # Label Encoding\n",
        "  label_encoder = LabelEncoder()\n",
        "  samples_df['Label_Encoded'] = label_encoder.fit_transform(samples_df['Label'])\n",
        "\n",
        "  return samples_df, label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "0HsmMhOoU0zy"
      },
      "outputs": [],
      "source": [
        "def combine_classes(samples_df, classes_to_combine, combined_class_name=\"Shrubs and Natural Grassland\"):\n",
        "  \"\"\"\n",
        "  Combines multiple classes into a single class.\n",
        "  \"\"\"\n",
        "  samples_df['Label'] = samples_df['Label'].replace(\n",
        "      dict.fromkeys(classes_to_combine, combined_class_name)\n",
        "  )\n",
        "\n",
        "  remaining_sample_classes = set(samples_df['Label'].unique())\n",
        "\n",
        "  for c in classes_to_combine:\n",
        "      if c in remaining_sample_classes:\n",
        "          raise ValueError(f\"Class '{c}' still present after combination.\")\n",
        "  if combined_class_name not in remaining_sample_classes:\n",
        "      raise ValueError(f\"Combined class '{combined_class_name}' not found after combination.\")\n",
        "\n",
        "  print(\"Classes combined successfully.\")\n",
        "  return samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "9Um_M9lIVNn-"
      },
      "outputs": [],
      "source": [
        "def remove_wetlands(samples_df):\n",
        "  # Extract Sample_num from the File column in samples_df\n",
        "  samples_df['Sample_num'] = samples_df['File'].str.split('_').str[0].astype(int)\n",
        "\n",
        "  # Identify all Sample_num values associated with \"Wetlands\" in samples_df\n",
        "  wetlands_samples = samples_df[samples_df['Label'].str.contains(\"Wetlands\", na=False)]['Sample_num'].unique()\n",
        "\n",
        "  # Remove rows in samples_df associated with the same Sample_num\n",
        "  samples_df = samples_df[~samples_df['Sample_num'].isin(wetlands_samples)]\n",
        "\n",
        "  # Verification\n",
        "  # Check that no rows with the removed Sample_num values are present in samples_df\n",
        "  assert not samples_df['Sample_num'].isin(wetlands_samples).any(), \"Wetlands samples still present in samples_df.\"\n",
        "\n",
        "  print(\"Successfully removed 'Wetlands' from the dataset.\")\n",
        "  return samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "PKKbqk5DLEGy"
      },
      "outputs": [],
      "source": [
        "def remove_planted_forest(samples_df):\n",
        "    # Extract Sample_num from the File column in samples_df\n",
        "    # (This step ensures that Sample_num is available and consistent)\n",
        "    samples_df['Sample_num'] = samples_df['File'].str.split('_').str[0].astype(int)\n",
        "\n",
        "    # Identify all Sample_num values associated with \"Planted Forest\" in samples_df\n",
        "    planted_forest_samples = samples_df[samples_df['Label'].str.contains(\"Planted Forest\", na=False)]['Sample_num'].unique()\n",
        "\n",
        "    # Remove rows in samples_df associated with these Sample_num values\n",
        "    samples_df = samples_df[~samples_df['Sample_num'].isin(planted_forest_samples)]\n",
        "\n",
        "    # Verification:\n",
        "    # Check that no rows with the removed Sample_num values are present in samples_df\n",
        "    assert not samples_df['Sample_num'].isin(planted_forest_samples).any(), \"Planted Forest samples still present in samples_df.\"\n",
        "\n",
        "    print(\"Successfully removed 'Planted Forest' from the dataset.\")\n",
        "    return samples_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6D43W9Kpm9N"
      },
      "source": [
        "### Training, Callbacks & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "yD84AW5W1vJJ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train_cat, X_test, y_test_cat, epochs, batch_size, checkpoint_path):\n",
        "  history = model.fit(X_train,\n",
        "      y_train_cat,\n",
        "      validation_data=(X_test, y_test_cat),\n",
        "      epochs=epochs,\n",
        "      batch_size=batch_size,\n",
        "      callbacks=define_callbacks(checkpoint_path)\n",
        "    )\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "_ClILCBqpt3M"
      },
      "outputs": [],
      "source": [
        "def define_callbacks(checkpoint_path):\n",
        "  # Make sure path ebds with .h5\n",
        "  assert checkpoint_path.endswith(\".h5\"), \"checkpoint_path should end with '.h5' for ModelCheckpoint.\"\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='val_accuracy',\n",
        "                                 patience=30,\n",
        "                                 mode='max',\n",
        "                                 verbose=1)\n",
        "\n",
        "  lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.2,\n",
        "                                   patience=20,\n",
        "                                   verbose=1,\n",
        "                                   min_lr=1e-6)\n",
        "\n",
        "  checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                               save_weights_only=True,\n",
        "                               monitor='val_accuracy',\n",
        "                               save_best_only=True,\n",
        "                               verbose=1)\n",
        "\n",
        "  return [checkpoint, early_stopping, lr_scheduler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "qRGEJhfpp43v"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test_cat):\n",
        "  test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=1)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "  return test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpyEF9RsE5Zq"
      },
      "source": [
        "### Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "Rm1zYvAJE7Cq"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(y_train):\n",
        "  classes = np.unique(y_train)\n",
        "  weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "  class_weights_dict = dict(enumerate(weights))\n",
        "\n",
        "  # Sort in desc. order\n",
        "  sorted_weights = sorted(class_weights_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  # Print formatted weights\n",
        "  print(\"\\nClass Weights:\")\n",
        "  for class_index, weight in sorted_weights:\n",
        "    # Decode class index to label\n",
        "    class_label = label_encoder.inverse_transform([class_index])[0]\n",
        "    print(f\"{class_label.ljust(22)} Weight: {str(weight).ljust(10)}\")\n",
        "\n",
        "  return class_weights_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "Xpw6UbT9HRyF"
      },
      "outputs": [],
      "source": [
        "def get_adjusted_class_weights(y_true, y_pred, label_encoder):\n",
        "    # Ensure that label_encoder.classes_ includes all possible classes\n",
        "    all_classes = np.union1d(np.unique(y_true), np.unique(y_pred))\n",
        "    missing_classes = set(all_classes) - set(range(len(label_encoder.classes_)))\n",
        "    if missing_classes:\n",
        "        raise ValueError(f\"Missing classes in label_encoder: {missing_classes}\")\n",
        "\n",
        "    # Compute F1-scores for each class\n",
        "    f1_scores = f1_score(y_true, y_pred, average=None, labels=np.unique(y_true))\n",
        "    classes = np.unique(y_true)\n",
        "\n",
        "    # Map class indices to F1-scores\n",
        "    class_f1_scores = dict(zip(classes, f1_scores))\n",
        "\n",
        "    # Invert F1-scores to get weights (add small epsilon to avoid division by zero)\n",
        "    epsilon = 1e-6\n",
        "    inverted_f1_scores = {cls: 1 / (score + epsilon) for cls, score in class_f1_scores.items()}\n",
        "\n",
        "    # Normalize weights to have a mean of 1\n",
        "    mean_weight = np.mean(list(inverted_f1_scores.values()))\n",
        "    normalized_weights = {cls: weight / mean_weight for cls, weight in inverted_f1_scores.items()}\n",
        "\n",
        "    # Map class indices to weights\n",
        "    class_weights_dict = {cls: normalized_weights[cls] for cls in classes}\n",
        "\n",
        "    # Print formatted weights\n",
        "    print(\"\\nAdjusted Class Weights:\")\n",
        "    for class_index in classes:\n",
        "      # Safely decode class labels\n",
        "      if class_index >= len(label_encoder.classes_):\n",
        "          class_label = f\"Unknown (Index {class_index})\"\n",
        "      else:\n",
        "          class_label = label_encoder.inverse_transform([class_index])[0]\n",
        "      weight = class_weights_dict[class_index]\n",
        "      print(f\"{class_label.ljust(22)} Weight: {weight:.3f}\")\n",
        "\n",
        "    return class_weights_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "JCCuIFBEFLDW"
      },
      "outputs": [],
      "source": [
        "def train_model_with_class_weights(model, X_train, y_train_cat, X_test, y_test_cat, class_weights, epochs=5, batch_size=32):\n",
        "  history = model.fit(\n",
        "    X_train,\n",
        "    y_train_cat,\n",
        "    validation_data=(X_test, y_test_cat),\n",
        "    class_weight=class_weights,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=define_callbacks(checkpoint_path),\n",
        "  )\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD4t_cmHEU_R"
      },
      "source": [
        "### Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "lshCmJ-ZPqdm"
      },
      "outputs": [],
      "source": [
        "def data_generator(X, y, batch_size, augmentation_methods):\n",
        "  \"\"\"\n",
        "  Generates batches of data with during training augmentations.\n",
        "  \"\"\"\n",
        "  while True:\n",
        "    # Shuffle the data at the beginning of each epoch\n",
        "    indices = np.random.permutation(len(X))\n",
        "    for start in range(0, len(X), batch_size):\n",
        "      end = min(start + batch_size, len(X))\n",
        "      batch_indices = indices[start:end]\n",
        "      # Extract the batch data\n",
        "      X_batch = X[batch_indices]\n",
        "      y_batch = y[batch_indices]\n",
        "      # Apply each augmentation method to the batch\n",
        "      for method in augmentation_methods:\n",
        "        X_batch = method(X_batch)\n",
        "      yield X_batch, y_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "niXy924wbqgY"
      },
      "outputs": [],
      "source": [
        "def augment_data(samples_df, oversample_images, target_size_per_class, frequency_cols):\n",
        "  augmented_samples = []\n",
        "\n",
        "  for class_label, sample_nums in oversample_images.items():\n",
        "    # Filter samples for the current class and specified sample_nums\n",
        "    class_df = samples_df[\n",
        "        (samples_df['Label'] == class_label) &\n",
        "        (samples_df['Sample_num'].isin(sample_nums))\n",
        "    ]\n",
        "\n",
        "    total_class_size = len(samples_df[samples_df['Label'] == class_label])\n",
        "    current_size = len(class_df)\n",
        "    target_size = target_size_per_class.get(class_label, total_class_size)\n",
        "    samples_needed = target_size - total_class_size\n",
        "\n",
        "    print(f\"\\nAugmenting class '{class_label}':\")\n",
        "    print(f\"Current total size: {total_class_size}, Target size: {target_size}, Samples needed: {samples_needed}\")\n",
        "\n",
        "    if samples_needed <= 0:\n",
        "      continue\n",
        "    if class_df.empty:\n",
        "      print(f\"No available samples to augment for class '{class_label}' from specified sample_nums.\")\n",
        "      continue\n",
        "\n",
        "    # Randomly sample spectra to augment\n",
        "    spectra_to_augment = class_df.sample(n=samples_needed, replace=True, random_state=42)\n",
        "\n",
        "    # Apply augmentation\n",
        "    augmented_spectra = spectra_to_augment[frequency_cols].apply(\n",
        "        lambda row: augment_spectrum(row.values), axis=1\n",
        "    )\n",
        "\n",
        "    # Create augmented DataFrame\n",
        "    augmented_df = spectra_to_augment.copy()\n",
        "    augmented_df[frequency_cols] = np.stack(augmented_spectra.values)\n",
        "    augmented_samples.append(augmented_df)\n",
        "\n",
        "  # Combine original and augmented data\n",
        "  augmented_samples_df = pd.concat([samples_df] + augmented_samples, ignore_index=True)\n",
        "\n",
        "  return augmented_samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "VXC-YokSbxLY"
      },
      "outputs": [],
      "source": [
        "def augment_spectrum(spectrum):\n",
        "  spectrum = add_gaussian_noise(spectrum)\n",
        "  spectrum = spectral_shift(spectrum)\n",
        "  # spectrum = spectral_scaling(spectrum)\n",
        "  # spectrum = random_band_dropout(spectrum)\n",
        "  return spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "yy_SIvVwOr39"
      },
      "outputs": [],
      "source": [
        "def add_gaussian_noise(spectrum, mean=0.0, std=0.01):\n",
        "  noise = np.random.normal(mean, std, spectrum.shape)\n",
        "  return spectrum + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "NTz7rdJ1OtJr"
      },
      "outputs": [],
      "source": [
        "def spectral_shift(spectrum, shift_range=0.01):\n",
        "  shift = np.random.uniform(-shift_range, shift_range, spectrum.shape)\n",
        "  return spectrum + shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "MYhJJt2SOu7Z"
      },
      "outputs": [],
      "source": [
        "def spectral_scaling(spectrum, scale_range=0.05):\n",
        "  scale = np.random.uniform(1 - scale_range, 1 + scale_range)\n",
        "  return spectrum * scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "4lSYFUjMOxPu"
      },
      "outputs": [],
      "source": [
        "def random_band_dropout(spectrum, dropout_rate=0.1):\n",
        "  spectrum = spectrum.copy()\n",
        "  num_bands = spectrum.shape[0]\n",
        "  num_dropout = int(num_bands * dropout_rate)\n",
        "  dropout_indices = np.random.choice(num_bands, num_dropout, replace=False)\n",
        "  spectrum[dropout_indices] = 0\n",
        "  return spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "mca5mwSaE-EA"
      },
      "outputs": [],
      "source": [
        "def train_model_with_generator(model, train_generator, steps_per_epoch, X_test, y_test_cat, epochs, callbacks):\n",
        "  history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=(X_test, y_test_cat),\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks\n",
        "  )\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivk1jcwZDrUV"
      },
      "source": [
        "### Oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rGd7SwwLJzG"
      },
      "source": [
        "#### Oversampling Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "vlzjedGoTEfL"
      },
      "outputs": [],
      "source": [
        "def split_data_by_image_with_oversampling(samples_df, oversample_images, test_size=0.2, random_state=42):\n",
        "  # Get all oversampling images\n",
        "  oversample_image_nums = set(sum(oversample_images.values(), []))\n",
        "\n",
        "  # Separate oversampling images from others\n",
        "  oversample_images_df = samples_df[samples_df['Sample_num'].isin(oversample_image_nums)]\n",
        "  remaining_images_df = samples_df[~samples_df['Sample_num'].isin(oversample_image_nums)]\n",
        "\n",
        "  # Get unique images and their labels from the remaining data\n",
        "  image_labels = remaining_images_df[['Sample_num', 'Label_Encoded']].drop_duplicates()\n",
        "\n",
        "  # Stratified splitting at the image level\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "  train_indices, test_indices = next(sss.split(image_labels['Sample_num'], image_labels['Label_Encoded']))\n",
        "\n",
        "  train_image_nums = image_labels['Sample_num'].iloc[train_indices]\n",
        "  test_image_nums = image_labels['Sample_num'].iloc[test_indices]\n",
        "\n",
        "  # Create training and testing DataFrames\n",
        "  train_df = remaining_images_df[remaining_images_df['Sample_num'].isin(train_image_nums)]\n",
        "  test_df = remaining_images_df[remaining_images_df['Sample_num'].isin(test_image_nums)]\n",
        "\n",
        "  # Add oversampling images back to the training set\n",
        "  train_df = pd.concat([train_df, oversample_images_df], ignore_index=True)\n",
        "\n",
        "  # Verify that all oversampling images are in the training set\n",
        "  oversample_images_in_test = oversample_image_nums.intersection(set(test_df['Sample_num'].unique()))\n",
        "  assert len(oversample_images_in_test) == 0, (\n",
        "      f\"Some oversampling images are in the test set: {oversample_images_in_test}\"\n",
        "  )\n",
        "\n",
        "  print(\"All oversampling images are in the training set.\")\n",
        "\n",
        "  return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "oMlkgNYmrsrb"
      },
      "outputs": [],
      "source": [
        "def oversample_to_target_size(samples_df, oversample_images, percentage=30):\n",
        "  \"\"\"\n",
        "  Oversample only the classes specified in oversample_images to a percentage of the largest class size.\n",
        "  Classes with a current size larger than the target size are left unchanged.\n",
        "  \"\"\"\n",
        "  oversampled_dfs = []\n",
        "\n",
        "  # Determine the size of the largest class\n",
        "  largest_class_size = samples_df['Label'].value_counts().max()\n",
        "  target_size_per_class = {\n",
        "    class_label: max(len(samples_df[samples_df['Label'] == class_label]),  # Ensure target size is >= current size\n",
        "                      int(largest_class_size * (percentage / 100)))\n",
        "    for class_label in oversample_images.keys()\n",
        "  }\n",
        "\n",
        "  for class_label in samples_df['Label'].unique():\n",
        "    class_df = samples_df[samples_df['Label'] == class_label]\n",
        "    current_class_size = len(class_df)\n",
        "\n",
        "    # Only oversample classes in oversample_images\n",
        "    if class_label in oversample_images:\n",
        "      target_size = target_size_per_class.get(class_label, current_class_size)\n",
        "    else:\n",
        "      target_size = current_class_size  # Do not modify other classes\n",
        "\n",
        "    samples_needed = target_size - current_class_size\n",
        "\n",
        "    print(f\"\\nProcessing class '{class_label}':\")\n",
        "    print(f\"Current size: {current_class_size}, Target size: {target_size}, Samples needed: {samples_needed}\")\n",
        "\n",
        "    if samples_needed > 0 and class_label in oversample_images:\n",
        "      # Use all pixels from the specified sample numbers\n",
        "      sample_nums = oversample_images[class_label]\n",
        "      oversample_candidates = samples_df[\n",
        "        (samples_df['Label'] == class_label) &\n",
        "        (samples_df['Sample_num'].isin(sample_nums))\n",
        "    ]\n",
        "\n",
        "      if oversample_candidates.empty:\n",
        "        print(f\"Warning: No oversample candidates available for class '{class_label}'. Skipping oversampling.\")\n",
        "        oversampled_class_df = class_df\n",
        "      else:\n",
        "        num_candidates = len(oversample_candidates)\n",
        "        if num_candidates < samples_needed:\n",
        "          print(f\"Note: Not enough candidates for class '{class_label}'. Sampling with replacement.\")\n",
        "        else:\n",
        "          print(f\"Sampling {samples_needed} samples from {num_candidates} candidates for class '{class_label}'.\")\n",
        "\n",
        "        # Sample with replacement\n",
        "        additional_samples = oversample_candidates.sample(\n",
        "          n=samples_needed, replace=True, random_state=42\n",
        "        )\n",
        "        oversampled_class_df = pd.concat([class_df, additional_samples], ignore_index=True)\n",
        "    else:\n",
        "        # Do not modify classes not in oversample_images\n",
        "        oversampled_class_df = class_df\n",
        "\n",
        "    # Verify class size matches target\n",
        "    final_class_size = len(oversampled_class_df)\n",
        "    if final_class_size != target_size and class_label in oversample_images:\n",
        "      print(f\"Warning: Class '{class_label}' final size ({final_class_size}) does not match target size ({target_size}).\")\n",
        "\n",
        "    oversampled_dfs.append(oversampled_class_df)\n",
        "\n",
        "\n",
        "\n",
        "  oversampled_df = pd.concat(oversampled_dfs, ignore_index=True)\n",
        "\n",
        "  # Final verification\n",
        "  print(\"\\nFinal class distributions after oversampling:\")\n",
        "  final_class_counts = oversampled_df['Label'].value_counts()\n",
        "  for class_label in oversample_images.keys():\n",
        "    target_size = target_size_per_class.get(class_label, final_class_counts[class_label])\n",
        "    actual_size = final_class_counts.get(class_label, 0)\n",
        "    if actual_size != target_size:\n",
        "      print(f\"Class '{class_label}' has {actual_size} samples, expected {target_size}.\")\n",
        "    else:\n",
        "      print(f\"Class '{class_label}' oversampled to target size {target_size}.\")\n",
        "\n",
        "  return oversampled_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "vHxa-hNGe8IV"
      },
      "outputs": [],
      "source": [
        "def verify_and_update_oversample_images(samples_df, oversample_images, target_count=100):\n",
        "  \"\"\"\n",
        "  Verifies and updates the oversample_images dictionary.\n",
        "  - Ensures original samples are found in samples_df.\n",
        "  - Identifies missing samples and prints details.\n",
        "  - Adds additional samples as needed to meet the target count per class.\n",
        "  \"\"\"\n",
        "  updated_oversample_images = {}\n",
        "\n",
        "  for class_label, sample_nums in oversample_images.items():\n",
        "    print(f\"\\nProcessing class: {class_label}\")\n",
        "\n",
        "    # Convert to sets for easier manipulation\n",
        "    current_samples = set(sample_nums)\n",
        "    available_samples = set(samples_df[samples_df['Label'] == class_label]['Sample_num'])\n",
        "\n",
        "    # Identify original samples found\n",
        "    found_original_samples = current_samples.intersection(available_samples)\n",
        "\n",
        "    # Identify missing samples\n",
        "    missing_from_samples_df = current_samples - set(samples_df['Sample_num'])\n",
        "\n",
        "    # Log missing sample details\n",
        "    if missing_from_samples_df:\n",
        "      print(f\"Missing from samples_df: {missing_from_samples_df}\")\n",
        "\n",
        "    # Calculate additional samples needed\n",
        "    additional_samples_needed = target_count - len(found_original_samples)\n",
        "    additional_samples = []\n",
        "    if additional_samples_needed > 0:\n",
        "      remaining_samples = list(available_samples - current_samples)\n",
        "      additional_samples = random.sample(\n",
        "          remaining_samples, min(len(remaining_samples), additional_samples_needed)\n",
        "      )\n",
        "\n",
        "    # Combine original found and additional samples\n",
        "    updated_samples = list(found_original_samples.union(additional_samples))\n",
        "\n",
        "    # Store updated samples in the dictionary\n",
        "    updated_oversample_images[class_label] = updated_samples\n",
        "\n",
        "    # Print summary for the class\n",
        "    print(f\"Original samples found: {len(found_original_samples)}\")\n",
        "    print(f\"Additional samples added: {len(additional_samples)}\")\n",
        "    print(f\"Total samples for class '{class_label}': {len(updated_samples)}\")\n",
        "\n",
        "  return updated_oversample_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nZuYwmhxzCw"
      },
      "source": [
        "#### Verification of Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "El7V1z__rqUZ"
      },
      "outputs": [],
      "source": [
        "def verify_oversampling(original_df, oversampled_df):\n",
        "  \"\"\"\n",
        "  Check out the distribution of classes after oversampling.\n",
        "  \"\"\"\n",
        "  # Reset indices for a resonable comparison\n",
        "  original_df = original_df.reset_index(drop=True)\n",
        "  oversampled_df = oversampled_df.reset_index(drop=True)\n",
        "\n",
        "  # Count class distributions before and after oversampling\n",
        "  original_counts = original_df['Label'].value_counts()\n",
        "  oversampled_counts = oversampled_df['Label'].value_counts()\n",
        "\n",
        "  # Align indices for consistent comparison\n",
        "  all_classes = set(original_counts.index).union(set(oversampled_counts.index))\n",
        "  original_counts = original_counts.reindex(all_classes, fill_value=0)\n",
        "  oversampled_counts = oversampled_counts.reindex(all_classes, fill_value=0)\n",
        "\n",
        "  # Print counts before and after\n",
        "  print(\"\\nClass distribution before oversampling:\")\n",
        "  print(original_counts)\n",
        "\n",
        "  print(\"\\nClass distribution after oversampling:\")\n",
        "  print(oversampled_counts)\n",
        "\n",
        "  # Plot distributions before and after oversampling\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  bar_width = 0.35  # Bar width for side-by-side comparison\n",
        "  x_labels = list(all_classes)\n",
        "  x = range(len(x_labels))\n",
        "\n",
        "  plt.bar(x, original_counts.values, width=bar_width, alpha=0.6, label='Before Oversampling')\n",
        "  plt.bar([i + bar_width for i in x], oversampled_counts.values, width=bar_width, alpha=0.6, label='After Oversampling')\n",
        "\n",
        "  plt.xlabel('Class Labels')\n",
        "  plt.ylabel('Number of Pixels')\n",
        "  plt.title('Class Distribution Before and After Oversampling')\n",
        "  plt.xticks([i + bar_width / 2 for i in x], x_labels, rotation=45)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Check the difference in distributions\n",
        "  print(\"\\nDifference in class distributions:\")\n",
        "  difference = oversampled_counts - original_counts\n",
        "  print(difference)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "mrXpgzfLzHdt"
      },
      "outputs": [],
      "source": [
        "def print_missing_samples_details(samples_df, oversample_images):\n",
        "  \"\"\"\n",
        "  Verifies if the sample numbers in oversample_images are present in both samples_df.\n",
        "  For missing sample numbers, prints all rows associated with them in samples_df.\n",
        "  \"\"\"\n",
        "  for class_label, sample_nums in oversample_images.items():\n",
        "    print(f\"\\nChecking missing samples for class: {class_label}\")\n",
        "    missing_found = False\n",
        "    for sample_num in sample_nums:\n",
        "      found_in_samples = sample_num in samples_df[\"Sample_num\"].values\n",
        "\n",
        "      if not found_in_samples:\n",
        "        missing_found = True\n",
        "        print(f\"\\nSample_num {sample_num} is missing:\")\n",
        "        print(f\"  - samples_df: {'FOUND' if found_in_samples else 'MISSING'}\")\n",
        "\n",
        "        if found_in_samples:\n",
        "          print(f\"\\nRows in samples_df for Sample_num {sample_num}:\")\n",
        "          print(samples_df[samples_df[\"Sample_num\"] == sample_num])\n",
        "\n",
        "    if not missing_found:\n",
        "        print(f\"All sample numbers for class '{class_label}' are present in both DataFrames.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "X_dpcHR4Q4li"
      },
      "outputs": [],
      "source": [
        "def verify_oversample_images_in_training_set(oversample_images, train_images):\n",
        "  \"\"\"\n",
        "  Verify that all images used for oversampling are in the training set.\n",
        "  Remove any images in `oversample_images` that are not in the training set.\n",
        "  \"\"\"\n",
        "  oversample_image_nums = set(sum(oversample_images.values(), []))\n",
        "  train_image_nums = set(train_images)\n",
        "\n",
        "  # Images used for oversampling that are not in the training set\n",
        "  oversample_images_in_test = oversample_image_nums - train_image_nums\n",
        "\n",
        "  if oversample_images_in_test:\n",
        "    print(\"Warning: The following oversampling images are in the testing set and should not be used for oversampling:\")\n",
        "    print(oversample_images_in_test)\n",
        "    # Remove these images from the oversample_images dictionary\n",
        "    for class_label, sample_nums in oversample_images.items():\n",
        "      oversample_images[class_label] = [num for num in sample_nums if num in train_image_nums]\n",
        "  else:\n",
        "    print(\"All oversampling images are in the training set.\")\n",
        "\n",
        "  return oversample_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "mqYUPNPgRwUy"
      },
      "outputs": [],
      "source": [
        "def verify_samples_in_samples_df(samples_df, oversample_images):\n",
        "  \"\"\"\n",
        "  Verify that the sample numbers in oversample_images are present in samples_df and correspond to the correct class labels.\n",
        "  \"\"\"\n",
        "  for class_label, sample_nums in oversample_images.items():\n",
        "    # Filter the DataFrame for matching samples and the specified class\n",
        "    matching_samples = samples_df[\n",
        "      (samples_df['Sample_num'].isin(sample_nums)) &\n",
        "      (samples_df['Label'] == class_label)\n",
        "    ]\n",
        "    # Assert that there are matching rows for the given sample numbers and class\n",
        "    try:\n",
        "      assert not matching_samples.empty, (\n",
        "          f\"No matching samples found in `samples_df` for class '{class_label}' with provided Sample_num values.\"\n",
        "      )\n",
        "      print(f\"Class: {class_label}, Provided Samples: {sample_nums}\")\n",
        "      print(f\"Matching Samples in `samples_df`: {len(matching_samples)}\")\n",
        "    except AssertionError as e:\n",
        "      print(f\"Assertion Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ7NlyxbICoz"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "0NE9EnNi2ICr"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(history, model_name=\"Model\"):\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
        "  plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='o')\n",
        "  plt.title(f'{model_name} Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
        "  plt.plot(history.history['val_loss'], label='Val Loss', marker='o')\n",
        "  plt.title(f'{model_name} Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "GcAIlA225y6h"
      },
      "outputs": [],
      "source": [
        "def plot_class_distribution(y, class_names, title=\"Class Distribution\"):\n",
        "  \"\"\"\n",
        "  Plots a histogram showing the distribution of samples across classes.\n",
        "\n",
        "  Parameters:\n",
        "  - y: array-like, labels (as strings or integers)\n",
        "  - class_names: list, names of the classes\n",
        "  - title: str, title for the plot\n",
        "  \"\"\"\n",
        "  # Convert to numpy array for consistency\n",
        "  y = np.array(y)\n",
        "\n",
        "  # Count occurrences of each class\n",
        "  if y.dtype.kind in ['i', 'u']:  # If numeric labels\n",
        "      class_counts = Counter(y)\n",
        "      counts = [class_counts.get(i, 0) for i in range(len(class_names))]\n",
        "  else:  # If string labels\n",
        "      class_counts = Counter(y)\n",
        "      counts = [class_counts.get(name, 0) for name in class_names]\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.bar(class_names, counts)\n",
        "  plt.xlabel('Class')\n",
        "  plt.ylabel('Number of Samples')\n",
        "  plt.title(title)\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.grid(axis='y')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "r4EDq-ZttV5t"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, label_encoder):\n",
        "  # Convert labels to integer indices\n",
        "  y_true_int = label_encoder.transform(y_true)\n",
        "  y_pred_int = label_encoder.transform(y_pred)\n",
        "\n",
        "  # Generate the confusion matrix\n",
        "  conf_matrix = confusion_matrix(y_true_int, y_pred_int, labels=range(len(label_encoder.classes_)))\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "              xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "  plt.xlabel(\"Predicted Labels\")\n",
        "  plt.ylabel(\"True Labels\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "Vns_sOkStZpB"
      },
      "outputs": [],
      "source": [
        "def plot_sample_predictions(X, y_true, y_pred, num_samples=5):\n",
        "  plt.figure(figsize=(15, num_samples * 2))\n",
        "  indices = np.random.choice(len(X), num_samples, replace=False)\n",
        "  for i, idx in enumerate(indices):\n",
        "    plt.subplot(num_samples, 1, i + 1)\n",
        "    plt.plot(X[idx].flatten(), label=\"Spectral Data\")\n",
        "    plt.title(f\"True Label: {y_true[idx]} | Predicted: {y_pred[idx]}\")\n",
        "    plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "VyBzHKl0ieNS"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, X_test, y_test_cat, label_encoder):\n",
        "  y_pred_probs = model.predict(X_test)\n",
        "  y_pred_int = np.argmax(y_pred_probs, axis=1)\n",
        "  y_true_int = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "  # Use inverse_transform only when you need class labels (strings)\n",
        "  y_pred_labels = label_encoder.inverse_transform(y_pred_int)\n",
        "  y_true_labels = label_encoder.inverse_transform(y_true_int)\n",
        "\n",
        "  return y_true_labels, y_pred_labels, y_true_int, y_pred_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "qBq4y6l-tgnS"
      },
      "outputs": [],
      "source": [
        "# Functon to load, preict, and visualze results for each image with color legend\n",
        "def load_predict_and_visualize(image_path, ground_truth_label, model, label_encoder):\n",
        "  with rasterio.open(image_path) as img:\n",
        "    img_data = img.read()\n",
        "    img_reshaped = img_data.transpose(1, 2, 0).reshape(-1, img_data.shape[0])\n",
        "    img_reshaped = img_reshaped.reshape(-1, img_data.shape[0], 1)\n",
        "\n",
        "    # Predict and reshape the output\n",
        "    img_pred = model.predict(img_reshaped)\n",
        "    img_pred_labels = np.argmax(img_pred, axis=1)\n",
        "    img_pred_labels_reshaped = img_pred_labels.reshape(img_data.shape[1], img_data.shape[2])\n",
        "\n",
        "  # Display the pred. and ground truth images\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
        "  ax[0].imshow(img_pred_labels_reshaped, cmap=\"viridis\", vmin=0, vmax=len(label_encoder.classes_) - 1)\n",
        "  ax[0].set_title(f\"Predicted Labels for Each Pixel\")\n",
        "  ax[0].axis(\"off\")\n",
        "  ground_truth_index = label_encoder.transform([ground_truth_label])[0]\n",
        "  ax[1].imshow(np.ones_like(img_pred_labels_reshaped) * ground_truth_index, cmap=\"viridis\", vmin=0, vmax=len(label_encoder.classes_) - 1)\n",
        "  ax[1].set_title(f\"Ground Truth Label: {ground_truth_label}\")\n",
        "  ax[1].axis(\"off\")\n",
        "\n",
        "  # Color legend for classes\n",
        "  class_names = label_encoder.classes_\n",
        "  num_classes = len(class_names)\n",
        "  cmap = plt.cm.get_cmap(\"viridis\", num_classes)\n",
        "  colors = cmap(range(num_classes))\n",
        "  handles = [mpatches.Patch(color=colors[i], label=f\"{i}: {class_names[i]}\") for i in range(num_classes)]\n",
        "  plt.legend(handles=handles, title=\"Class Labels\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "rdxAfYeM06RD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def compute_and_plot_image_level_confusion_matrix_with_returns(model, test_df, label_encoder):\n",
        "  \"\"\"\n",
        "  Computes image-level predictions by aggregating pixel-level predictions and\n",
        "  generates a normalized confusion matrix and classification report.\n",
        "\n",
        "  Parameters:\n",
        "  - model: Trained model for prediction.\n",
        "  - test_df: DataFrame containing test samples with features and labels.\n",
        "  - label_encoder: LabelEncoder instance used for encoding labels.\n",
        "\n",
        "  Returns:\n",
        "  - image_accuracy: Accuracy at the image level.\n",
        "  - image_conf_matrix_normalized: Normalized confusion matrix at the image level.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract feature columns for prediction\n",
        "  frequency_cols = [col for col in test_df.columns if 'frq' in col]\n",
        "  X_test = test_df[frequency_cols].values\n",
        "  X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # Reshape for Conv1D\n",
        "\n",
        "  # Predict pixel-level probabilities\n",
        "  y_pred_probs = model.predict(X_test, verbose=0)\n",
        "\n",
        "  # Convert probabilities to predicted class indices\n",
        "  test_df['Predicted_Pixel_Label'] = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "  # Aggregate predictions per `Sample_num` by majority vote\n",
        "  image_predictions = (\n",
        "    test_df.groupby('Sample_num')['Predicted_Pixel_Label']\n",
        "    .apply(lambda x: np.bincount(x).argmax())  # Majority vote\n",
        "    .reset_index()\n",
        "    .rename(columns={'Predicted_Pixel_Label': 'Predicted_Image_Label'})\n",
        "  )\n",
        "\n",
        "  # Use `Ground_Truth` from `test_df` for comparison\n",
        "  ground_truth_mapping = test_df[['Sample_num', 'Label']].drop_duplicates()\n",
        "  image_predictions = image_predictions.merge(\n",
        "    ground_truth_mapping,\n",
        "    on='Sample_num',\n",
        "    how='left'\n",
        "  )\n",
        "\n",
        "  # Encode ground truth and predicted image labels\n",
        "  image_predictions['Encoded_Label'] = label_encoder.transform(image_predictions['Label'])\n",
        "  image_predictions['Encoded_Predicted_Label'] = image_predictions['Predicted_Image_Label']\n",
        "\n",
        "  # Compute image-level accuracy\n",
        "  correct_predictions = image_predictions['Encoded_Label'] == image_predictions['Encoded_Predicted_Label']\n",
        "  image_accuracy = correct_predictions.mean()\n",
        "  print(f\"Image-Level Accuracy: {image_accuracy:.4f}\")\n",
        "\n",
        "  # Compute normalized confusion matrix\n",
        "  image_conf_matrix = confusion_matrix(\n",
        "    image_predictions['Encoded_Label'],\n",
        "    image_predictions['Encoded_Predicted_Label'],\n",
        "    labels=range(len(label_encoder.classes_)),\n",
        "    normalize='true'  # Normalize rows to sum to 1\n",
        "  )\n",
        "\n",
        "  # Plot normalized confusion matrix\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  sns.heatmap(image_conf_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "              xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "\n",
        "  plt.xlabel(\"Predicted Labels\")\n",
        "  plt.ylabel(\"True Labels\")\n",
        "  plt.title(\"Image-Level Normalized Confusion Matrix\")\n",
        "  plt.show()\n",
        "\n",
        "  # Print classification report\n",
        "  y_true_images = label_encoder.inverse_transform(image_predictions['Encoded_Label'])\n",
        "  y_pred_images = label_encoder.inverse_transform(image_predictions['Encoded_Predicted_Label'])\n",
        "\n",
        "  print(\"Image-Level Classification Report:\")\n",
        "  print(classification_report(y_true_images, y_pred_images, target_names=label_encoder.classes_))\n",
        "\n",
        "  return image_accuracy, image_conf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "5rLB36PmQje0"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy_and_confusion_matrices(\n",
        "  total_correct_before, total_correct_after, total_pixels,\n",
        "  y_true_pixels, y_pred_pixels_before, y_pred_pixels_after,\n",
        "  y_true_images, y_pred_images_before, y_pred_images_after, label_encoder):\n",
        "\n",
        "  # Get overall per-pixel accuracy before and after morphology\n",
        "  accuracy_before = total_correct_before / total_pixels if total_pixels > 0 else 0\n",
        "  accuracy_after = total_correct_after / total_pixels if total_pixels > 0 else 0\n",
        "\n",
        "  print(f\"\\nOverall per-pixel accuracy before morphology: {accuracy_before:.4f}\")\n",
        "  print(f\"Overall per-pixel accuracy after morphology: {accuracy_after:.4f}\")\n",
        "\n",
        "  # MAke confusion matrices\n",
        "  conf_matrix_pixels_after = confusion_matrix(y_true_pixels, y_pred_pixels_after, labels=range(len(label_encoder.classes_)))\n",
        "\n",
        "  # Plot confusion matrices\n",
        "  plot_confusion_matrix_from_confusion_matrix(conf_matrix_pixels_after, label_encoder, title=\"Per-pixel Confusion Matrix After Morphology\")\n",
        "\n",
        "  # Print classification reports\n",
        "  print(\"\\nPer-pixel Classification Report After Morphology:\")\n",
        "  print(classification_report(y_true_pixels, y_pred_pixels_after, target_names=label_encoder.classes_))\n",
        "\n",
        "  # Return metrics\n",
        "  return {\n",
        "    'accuracy_before': accuracy_before,\n",
        "    'accuracy_after': accuracy_after,\n",
        "    'conf_matrix_pixels_after': conf_matrix_pixels_after,\n",
        "    'conf_matrix_images_after': conf_matrix_images_after\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlKQP_x7Qmh_"
      },
      "source": [
        "# 1D - CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey36DfFIzSOi"
      },
      "source": [
        "## These cells contain functions to define the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGf_Vwn4tDeT"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeToD6FztFMi"
      },
      "outputs": [],
      "source": [
        "# Define a 1D CNN model architecture for multi-class classification.\n",
        "def get_cnn_model(input_shape, num_classes, wd, drop_rate, learning_rate):\n",
        "  model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    Conv1D(512, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(drop_rate),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(drop_rate),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(drop_rate),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-1KIjFCR9rt"
      },
      "source": [
        "### Morphology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqwRqwSESgDb"
      },
      "outputs": [],
      "source": [
        "def generate_colors_for_labels_combined_classes(label_encoder):\n",
        "  label_to_color = {\n",
        "    \"Annual Crops\": \"#DA70D6\",        # Orchid\n",
        "    \"Built-up\": \"#FF0000\",            # Red\n",
        "    \"Consolidated Barren\": \"#555555\", # Dark Gray\n",
        "    \"Natural Wooded Land\": \"#006400\", # Dark Green\n",
        "    \"Permanent Crops\": \"#87CEEB\",     # Sky Blue\n",
        "    \"Planted Forest\": \"#8A2BE2\",      # Purple\n",
        "    \"Shrubs and Natural Grassland\": \"#32CD32\",              # Lime Green\n",
        "    \"Unconsolidated Barren\": \"#8B4513\",# Saddle Brown\n",
        "    \"Waterbodies\": \"#0000FF\",         # Blue\n",
        "  }\n",
        "\n",
        "  # Map colors based on label_encoder.classes_\n",
        "  colors = [label_to_color[label] for label in label_encoder.classes_]\n",
        "  return {i: colors[i] for i in range(len(label_encoder.classes_))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCQNWMOtRSRh"
      },
      "outputs": [],
      "source": [
        "def generate_colors_for_labels(label_encoder):\n",
        "  label_to_color = {\n",
        "    \"Annual Crops\": \"#DA70D6\",        # Orchid\n",
        "    \"Built-up\": \"#FF0000\",            # Red\n",
        "    \"Consolidated Barren\": \"#555555\", # Dark Gray\n",
        "    \"Natural Grassland\": \"#9ACD32\",   # Yellow-Green\n",
        "    \"Natural Wooded Land\": \"#006400\", # Dark Green\n",
        "    \"Permanent Crops\": \"#87CEEB\",     # Sky Blue\n",
        "    \"Planted Forest\": \"#8A2BE2\",      # Purple\n",
        "    \"Shrubs\": \"#32CD32\",              # Lime Green\n",
        "    \"Unconsolidated Barren\": \"#8B4513\",# Saddle Brown\n",
        "    \"Waterbodies\": \"#0000FF\",         # Blue\n",
        "  }\n",
        "\n",
        "  # Map colors based on label_encoder.classes_\n",
        "  colors = [label_to_color[label] for label in label_encoder.classes_]\n",
        "  return {i: colors[i] for i in range(len(label_encoder.classes_))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Cef2Ead_CbL"
      },
      "outputs": [],
      "source": [
        "def get_mode_or_default(values, default=-1):\n",
        "  \"\"\"\n",
        "  Returns the mode of the given array or a default value if the array is empty.\n",
        "  \"\"\"\n",
        "  if len(values) == 0:\n",
        "    return default\n",
        "\n",
        "  mode_result = stats.mode(values, axis=None)\n",
        "\n",
        "  # Handle both scalar and array-like mode results\n",
        "  if hasattr(mode_result.mode, \"__len__\") and len(mode_result.mode) > 0:\n",
        "    return mode_result.mode[0]\n",
        "  elif not hasattr(mode_result.mode, \"__len__\") and mode_result.mode is not None:\n",
        "    return mode_result.mode\n",
        "  else:\n",
        "    return default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Lvx3Fx_Ede"
      },
      "outputs": [],
      "source": [
        "def clean_img_pos_column(samples_df):\n",
        "  \"\"\"\n",
        "  Makes sure that the img_pos column contains tuples of integers (it was strings lol).\n",
        "  \"\"\"\n",
        "  def parse_img_pos(value):\n",
        "    if isinstance(value, str):\n",
        "      # Remove brackets and split by comma\n",
        "      return tuple(map(int, value.strip('()').split(',')))\n",
        "    elif isinstance(value, (tuple, list)):\n",
        "      return tuple(value)\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid img_pos value: {value}\")\n",
        "\n",
        "  samples_df['img_pos'] = samples_df['img_pos'].apply(parse_img_pos)\n",
        "  return samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1XQrNuM_G0W"
      },
      "outputs": [],
      "source": [
        "def predict_labels_for_sample(sample_pixels, model, label_encoder):\n",
        "  spectral_data = sample_pixels[[f'frq{i}' for i in range(373)]].values\n",
        "  spectral_data = spectral_data.reshape(-1, 373, 1)\n",
        "  predictions = model.predict(spectral_data)\n",
        "  predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "  unique_predicted_labels = np.unique(predicted_labels)\n",
        "  print(f\"Unique predicted labels: {unique_predicted_labels}\")\n",
        "\n",
        "  max_label_index = predicted_labels.max()\n",
        "  num_classes = len(label_encoder.classes_)\n",
        "  print(f\"Max label index: {max_label_index}\")\n",
        "  print(f\"Number of classes in label_encoder: {num_classes}\")\n",
        "\n",
        "  # Check if labels are out of bounds\n",
        "  if max_label_index >= num_classes:\n",
        "    raise ValueError(f\"Predicted label index {max_label_index} exceeds the number of classes in label_encoder ({num_classes}).\")\n",
        "\n",
        "  # Get image shape from 'img_pos' (thanks sam)\n",
        "  max_row = sample_pixels['img_pos'].apply(lambda x: x[0]).max()\n",
        "  max_col = sample_pixels['img_pos'].apply(lambda x: x[1]).max()\n",
        "  img_shape = (max_row + 1, max_col + 1)\n",
        "\n",
        "  # Create empty grid for image\n",
        "  full_image = np.zeros(img_shape, dtype=int)\n",
        "\n",
        "  # Map pred labels to positions using 'img_pos'\n",
        "  for idx, label in zip(sample_pixels.index, predicted_labels):\n",
        "    pos = sample_pixels.at[idx, 'img_pos']\n",
        "    if 0 <= pos[0] < img_shape[0] and 0 <= pos[1] < img_shape[1]:\n",
        "        full_image[pos[0], pos[1]] = label\n",
        "    else:\n",
        "        print(f\"Warnig: Pos. {pos} out of bounds for shape {img_shape}. Skipping...\")\n",
        "\n",
        "  return full_image, img_shape, predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1tSO4o__IlP"
      },
      "outputs": [],
      "source": [
        "def apply_morphology_majority_only(full_image, window_size=3):\n",
        "  \"\"\"\n",
        "  Apply dilation only to majority class in image.\n",
        "  window_size: Size of element for dilation.\n",
        "  filtered_image: Image after applying dilation.\n",
        "  \"\"\"\n",
        "  from skimage.morphology import dilation, square\n",
        "  import numpy as np\n",
        "\n",
        "  # Dilation square\n",
        "  selem = square(window_size)\n",
        "\n",
        "  # Copy original img to preserve other classes\n",
        "  filtered_image = np.copy(full_image)\n",
        "\n",
        "  # Get majority class\n",
        "  unique_labels, label_counts = np.unique(full_image, return_counts=True)\n",
        "  majority_class = unique_labels[np.argmax(label_counts)]\n",
        "\n",
        "  print(f\"Majority class: {majority_class} with {np.max(label_counts)} pixels\")\n",
        "\n",
        "  # Get mask for majority class\n",
        "  majority_mask = (full_image == majority_class)\n",
        "\n",
        "  # Dilate majority class\n",
        "  dilated_mask = dilation(majority_mask, selem)\n",
        "\n",
        "  # Update filtered image: Only modify pixels where dilation happens\n",
        "  filtered_image[dilated_mask] = majority_class\n",
        "\n",
        "  return filtered_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT_AWU2G_KUf"
      },
      "outputs": [],
      "source": [
        "def visualize_results(full_image, filtered_image, sample_num, ground_truth_label, label_encoder, color_mapping, colormap):\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
        "  ax[0].imshow(full_image, cmap=colormap, vmin=0, vmax=len(label_encoder.classes_) - 1)\n",
        "  ax[0].set_title(f\"Original Pred Labels\\nSample_num: {sample_num}\\nGround Truth: {ground_truth_label}\")\n",
        "  ax[0].axis(\"off\")\n",
        "\n",
        "  ax[1].imshow(filtered_image, cmap=colormap, vmin=0, vmax=len(label_encoder.classes_) - 1)\n",
        "  ax[1].set_title(f\"Filtered Pred Labels\\nSample_num: {sample_num}\\nGround Truth: {ground_truth_label}\")\n",
        "  ax[1].axis(\"off\")\n",
        "\n",
        "  # legend for color mapping\n",
        "  unique_labels = np.unique(filtered_image)\n",
        "  handles = []\n",
        "  for label in unique_labels:\n",
        "    try:\n",
        "      label_name = label_encoder.inverse_transform([label])[0]\n",
        "      color = color_mapping[label]\n",
        "      handles.append(mpatches.Patch(color=color, label=f\"{label}: {label_name}\"))\n",
        "    except IndexError:\n",
        "      print(f\"Label index {label} out of bounds for label_encoder.classes_. Skipping..\")\n",
        "    except KeyError:\n",
        "      print(f\"Label index {label} not found in color_mapping. Skipping...\")\n",
        "\n",
        "  ax[2].axis(\"off\")\n",
        "  fig.legend(handles=handles, loc='center', bbox_to_anchor=(1.1, 0.5), title=\"Label Color Mapping\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLRRpHyI_LeO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_from_confusion_matrix(conf_matrix, label_encoder, title=\"Confusion Matrix\"):\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "              xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "  plt.xlabel(\"Predicted Labels\")\n",
        "  plt.ylabel(\"True Labels\")\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6zJCcrRPQqx"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "### THIS TAKES A LONG TIME AS ONE WOULD IMAGINE! ####\n",
        "#####################################################\n",
        "def process_samples_with_morphology(samples_df, model, label_encoder, color_mapping, window_size=3):\n",
        "  # 'img_pos' column is \"cleaned\" (turning to int)\n",
        "  samples_df = clean_img_pos_column(samples_df)\n",
        "\n",
        "  # Debugging to confirm column transformation\n",
        "  print(\"Checking 'img_pos' column after cleaning:\")\n",
        "  print(samples_df['img_pos'].apply(type).value_counts())\n",
        "\n",
        "\n",
        "  # Init total count for acc.\n",
        "  total_correct_before = 0\n",
        "  total_correct_after = 0\n",
        "  total_pixels = 0\n",
        "\n",
        "  # Using for the confusion matrices\n",
        "  y_true_pixels = []\n",
        "  y_pred_pixels_before = []\n",
        "  y_pred_pixels_after = []\n",
        "\n",
        "  y_true_images = []\n",
        "  y_pred_images_before = []\n",
        "  y_pred_images_after = []\n",
        "\n",
        "  # Init counter to track num of img's\n",
        "  image_counter = 0\n",
        "\n",
        "  # Make a color mapping\n",
        "  colormap = ListedColormap([color_mapping[label] for label in sorted(color_mapping.keys())])\n",
        "\n",
        "  # Process each sample based on 'Sample_num'\n",
        "  unique_samples = samples_df['Sample_num'].unique()\n",
        "\n",
        "  for sample_num in unique_samples:\n",
        "    image_counter += 1  # Increment image counter\n",
        "\n",
        "    # Get rows corresponding to the current 'Sample_num'\n",
        "    sample_pixels = samples_df[samples_df['Sample_num'] == sample_num]\n",
        "\n",
        "    # Makin syre there are rows to process\n",
        "    if sample_pixels.empty:\n",
        "      print(f\"Sample_num {sample_num} has no associated data. Skipping.\")\n",
        "      continue\n",
        "\n",
        "    # Get ground truth label\n",
        "    ground_truth_label = sample_pixels.iloc[0]['Ground_Truth']\n",
        "    ground_truth_encoded = label_encoder.transform([ground_truth_label])[0]\n",
        "    print(f\"\\nProcessing Sample_num: {sample_num}\")\n",
        "    print(f\"  - Ground Truth Label: {ground_truth_label}\")\n",
        "\n",
        "    # Predict labels and make the full_image\n",
        "    try:\n",
        "      full_image, img_shape, predicted_labels = predict_labels_for_sample(sample_pixels, model, label_encoder)\n",
        "    except Exception as e:\n",
        "      print(f\"  - Error procesig Sample_num {sample_num}: {e}\")\n",
        "      continue\n",
        "\n",
        "    total_pixels_in_image = full_image.size\n",
        "    total_pixels += total_pixels_in_image\n",
        "\n",
        "    # Collect per-pixel predictions before morphology\n",
        "    valid_predicted_labels_before = full_image.flatten()\n",
        "    y_true_pixels.extend([ground_truth_encoded] * total_pixels_in_image)\n",
        "    y_pred_pixels_before.extend(valid_predicted_labels_before)\n",
        "\n",
        "    # Apply morphology\n",
        "    filtered_image = apply_morphology_majority_only(full_image, window_size)\n",
        "\n",
        "    # Collect per-pixel predictions after morphology\n",
        "    valid_predicted_labels_after = filtered_image.flatten()\n",
        "    y_pred_pixels_after.extend(valid_predicted_labels_after)\n",
        "\n",
        "    # Compute per-pixel accuracy before morphology\n",
        "    correct_predictions_before = np.sum(valid_predicted_labels_before == ground_truth_encoded)\n",
        "    total_correct_before += correct_predictions_before\n",
        "\n",
        "    # Compute per-pixel accuracy after morphology\n",
        "    correct_predictions_after = np.sum(valid_predicted_labels_after == ground_truth_encoded)\n",
        "    total_correct_after += correct_predictions_after\n",
        "\n",
        "    # Compute per-image predictions before and after morphology\n",
        "    pred_label_before = get_mode_or_default(valid_predicted_labels_before)\n",
        "    pred_label_after = get_mode_or_default(valid_predicted_labels_after)\n",
        "\n",
        "    y_true_images.append(ground_truth_encoded)\n",
        "    y_pred_images_before.append(pred_label_before)\n",
        "    y_pred_images_after.append(pred_label_after)\n",
        "\n",
        "    # Show results every 10th image or the last image (Sedel you can change this for more!)\n",
        "    if image_counter % 10 == 0 or image_counter == len(unique_samples):\n",
        "      visualize_results(\n",
        "        full_image, filtered_image, sample_num, ground_truth_label,\n",
        "        label_encoder, color_mapping, colormap\n",
        "      )\n",
        "\n",
        "  results = {\n",
        "      'total_correct_before': total_correct_before,\n",
        "      'total_correct_after': total_correct_after,\n",
        "      'total_pixels': total_pixels,\n",
        "      'y_true_pixels': y_true_pixels,\n",
        "      'y_pred_pixels_before': y_pred_pixels_before,\n",
        "      'y_pred_pixels_after': y_pred_pixels_after,\n",
        "      'y_true_images': y_true_images,\n",
        "      'y_pred_images_before': y_pred_images_before,\n",
        "      'y_pred_images_after': y_pred_images_after,\n",
        "      'label_encoder': label_encoder\n",
        "  }\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGBJzTbJmsg1"
      },
      "source": [
        "## 1D - Training CNN\n",
        "- Not using Oversampling or Class Weights\n",
        "- Combining Natural Grassland with Shrubs\n",
        "- Using Post-Processing (Morphology)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yARuJ0qzrN2y"
      },
      "source": [
        "### PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YHh-i6mmrN2y",
        "outputId": "04252e3a-3d57-4da7-e31e-fc9e2de55934"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "samples_df = pd.read_csv(samples_path)\n",
        "\n",
        "########################  TEMPORARILY COMBINE SHRUBS & GRASSLAND  #########################\n",
        "samples_df = combine_classes(samples_df, ['Natural Grassland', 'Shrubs'], 'Shrubs and Natural Grassland')\n",
        "########################  TEMPORARILY COMBINE SHRUBS & GRASSLAND  #########################\n",
        "\n",
        "########################  TEMPORARILY REMOVE WETLANDS  #########################\n",
        "samples_df = remove_wetlands(samples_df)\n",
        "########################  TEMPORARILY REMOVE WETLANDS  #########################\n",
        "\n",
        "# Preprocess Data\n",
        "samples_df, label_encoder = preprocess_data(samples_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "4HCpFS8JTHNn",
        "outputId": "1fc7adb9-e7ad-42f6-ad82-e139c1ddfd00"
      },
      "outputs": [],
      "source": [
        "samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T3fmGL9rN2z"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with unique images and their labels\n",
        "image_samples_df = samples_df[['Sample_num', 'Label_Encoded']].drop_duplicates()\n",
        "\n",
        "# Stratified splitting at the image level\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_indices, test_indices = next(sss.split(image_samples_df['Sample_num'], image_samples_df['Label_Encoded']))\n",
        "\n",
        "# Get the Sample_num for training and testing\n",
        "train_sample_nums = image_samples_df['Sample_num'].iloc[train_indices]\n",
        "test_sample_nums = image_samples_df['Sample_num'].iloc[test_indices]\n",
        "\n",
        "# Create training and testing DataFrames\n",
        "train_df = samples_df[samples_df['Sample_num'].isin(train_sample_nums)]\n",
        "test_df = samples_df[samples_df['Sample_num'].isin(test_sample_nums)]\n",
        "\n",
        "# Get the frequency columns `frq0` to `frq372`\n",
        "frequency_cols = [col for col in samples_df.columns if 'frq' in col]\n",
        "\n",
        "# Extract features and samples for training and testing sets\n",
        "X_train = train_df[frequency_cols].values\n",
        "y_train = train_df['Label_Encoded'].values\n",
        "X_test = test_df[frequency_cols].values\n",
        "y_test = test_df['Label_Encoded'].values\n",
        "\n",
        "# Model parameters\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Convert samples to One-Hot Encoding\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Add a channel dimension for Conv1D\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0MatYzprN2z"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train_cat.shape, y_test_cat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlMgwHFF6D_s"
      },
      "source": [
        "### Define Model (Loading from weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQMiCItb6D_t"
      },
      "outputs": [],
      "source": [
        "# Create new instance of model\n",
        "model = get_cnn_model(input_shape, num_classes, wd=1e-6, drop_rate=0.3, learning_rate=.0001)\n",
        "\n",
        "# Load weights\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiheAiQysCSJ"
      },
      "source": [
        "### Define & Train CNN (If not loading from wieghts!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "djQpyOvesCSK"
      },
      "outputs": [],
      "source": [
        "# # Define callbacks\n",
        "# callbacks = define_callbacks(checkpoint_path)\n",
        "\n",
        "# # Initialize CNN model\n",
        "# model = get_cnn_model(input_shape, num_classes, wd=1e-6, drop_rate=0.5, learning_rate=.0001)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tjd0jXisCSL"
      },
      "outputs": [],
      "source": [
        "# # Train the model\n",
        "history = train_model(model, X_train, y_train_cat, X_test, y_test_cat, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcokXGwHsCSL"
      },
      "source": [
        "### Test Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOwcwzA1sCSL"
      },
      "outputs": [],
      "source": [
        "# Get the testing Acc. after training\n",
        "test_accuracy = evaluate_model(model, X_test, y_test_cat)\n",
        "print(\"Final Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkFL7_8YsCSL"
      },
      "source": [
        "### Post-Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06_O252lsCSL"
      },
      "outputs": [],
      "source": [
        "# Get predictions on the test set\n",
        "y_test_labels, y_pred_labels, _, _  = get_predictions(model, X_test, y_test_cat, label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw7aNyQ0fzf_"
      },
      "outputs": [],
      "source": [
        "# Plotting the training curves\n",
        "plot_training_curves(history, \"1D CNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGW2W9y4sCSM"
      },
      "outputs": [],
      "source": [
        "# Create a CM\n",
        "plot_confusion_matrix(y_test_labels, y_pred_labels, label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDRBEbV_sCSL"
      },
      "outputs": [],
      "source": [
        "# Get the Classification Report\n",
        "print(classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bZ2u3Ag_sCSM"
      },
      "outputs": [],
      "source": [
        "# # Load and visualize individual images\n",
        "# filtered_samples_df = samples_df[['Sample_num', 'Label', 'File']].drop_duplicates()\n",
        "# for index, row in filtered_samples_df.iterrows():\n",
        "#   image_path = os.path.join(images_path, row['File'])\n",
        "#   if os.path.exists(image_path):\n",
        "#     load_predict_and_visualize(image_path, row['Label'], model, label_encoder)\n",
        "#   else:\n",
        "#     print(f\"File not found: {image_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EERyEFk_sCSM"
      },
      "source": [
        "### Plot the Image-Level Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_ZlTMilhVoa"
      },
      "outputs": [],
      "source": [
        "image_accuracy, image_conf_matrix = compute_and_plot_image_level_confusion_matrix(model, test_df, label_encoder):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-wHZ_yrtkl"
      },
      "source": [
        "### Morphology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7jmewUurtkm"
      },
      "outputs": [],
      "source": [
        "## Remeber to define your model first! Just load from some weights to make it easy...\n",
        "## NOTE: This takes a long time to run!\n",
        "color_mapping = generate_colors_for_labels_combined_classes(label_encoder)\n",
        "\n",
        "results = process_samples_with_morphology(\n",
        "  test_df,\n",
        "  model,\n",
        "  label_encoder,\n",
        "  color_mapping,\n",
        "  window_size=3,  # Using smallest window possbile lol\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ehJfuDrtkm"
      },
      "source": [
        "### Post-Processing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leQpkusdiOjj"
      },
      "outputs": [],
      "source": [
        "metrics = compute_accuracy_and_confusion_matrices(**results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwWci_Zprtkm"
      },
      "outputs": [],
      "source": [
        "# Extrat per-pixel true labels and predicted labels after morphogy\n",
        "y_true_pixels = results['y_true_pixels']  # Ground truth labels for pixels\n",
        "y_pred_pixels_after = results['y_pred_pixels_after']  # Predicted labels after morphology\n",
        "\n",
        "# Get per-pixel accuracy after morphology\n",
        "accuracy_after_morphology = np.mean(np.array(y_true_pixels) == np.array(y_pred_pixels_after))\n",
        "print(f\"Per-pixel Accuracy After Morphology: {accuracy_after_morphology:.4f}\")\n",
        "\n",
        "# Get classification report after morphology\n",
        "print(\"Per-pixel Classification Report After Morphology:\")\n",
        "print(classification_report(y_true_pixels, y_pred_pixels_after, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6iARxuMrtkm"
      },
      "outputs": [],
      "source": [
        "# Extract per-image true labels and predicted labels after morphology\n",
        "y_true_images = results['y_true_images']  # Ground truth labels for images\n",
        "y_pred_images_after = results['y_pred_images_after']  # Predicted labels after morphology\n",
        "\n",
        "# Get per-image classification report\n",
        "print(\"Per-image Classification Report After Morphology:\")\n",
        "print(classification_report(y_true_images, y_pred_images_after, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMzpqg9Drtkm"
      },
      "outputs": [],
      "source": [
        "# fget confusion matrix using post-processed predictions\n",
        "conf_matrix_after = confusion_matrix(\n",
        "  results['y_true_pixels'],  # True labels\n",
        "  results['y_pred_pixels_after'],  # Post-processed predicted labels\n",
        "  labels=range(len(label_encoder.classes_))\n",
        ")\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(\n",
        "  conf_matrix_after, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "  xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_\n",
        ")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix After Morphology\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxh3n5Rgrtkm"
      },
      "outputs": [],
      "source": [
        "# Use post-processed per-image predictions\n",
        "image_predictions_after = pd.DataFrame({\n",
        "    'Encoded_Ground_Truth': results['y_true_images'],\n",
        "    'Encoded_Predicted_Label': results['y_pred_images_after']\n",
        "})\n",
        "\n",
        "# Geet normalized confusion matrix\n",
        "cm_normalized_after = confusion_matrix(\n",
        "    image_predictions_after['Encoded_Ground_Truth'],\n",
        "    image_predictions_after['Encoded_Predicted_Label'],\n",
        "    labels=range(len(label_encoder.classes_)),\n",
        "    normalize='true'\n",
        ")\n",
        "\n",
        "# Plot the normalized confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized_after, display_labels=label_encoder.classes_)\n",
        "\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=45, ax=ax, colorbar=True)\n",
        "ax.set_xlabel(\"Predicted Label\", fontsize=14, labelpad=15)\n",
        "ax.set_ylabel(\"True Label\", fontsize=14, labelpad=15)\n",
        "ax.set_title(\"Image-Level Confusion Matrix After Morphology\", fontsize=16, pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfqFEusUQurj"
      },
      "source": [
        "# 2D - CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL_6K7mBtfkW"
      },
      "source": [
        "### Explanation of Model Differences\n",
        " - **MicroCNN & MacroCNN:** These models use PCA components (specifically PC2, PC3, PC4) as the three input channels.\n",
        "   - The images are spatial arrays of pixels derived from the PCA-transformed spectral data.\n",
        "   - MicroCNN is a smaller network with fewer layers and parameters\n",
        "   - MacroCNN is deeper and includes more layers and dropout for potentially better generalization.\n",
        "\n",
        " - **RGB CNN:** Uses three selected frequency bands directly as input channels, analogous to standard RGB images.\n",
        "  - This approach tests the model's ability to classify based on raw spectral channels chosen to mimic an RGB representation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " In summary:\n",
        " - MicroCNN & MacroCNN inputs: PCA images (PC2, PC3, PC4).\n",
        " - RGB CNN input: Selected raw spectral bands arranged as RGB channels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubz3aOAEuSNW"
      },
      "source": [
        "### These cells contain functions to define the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXmeVHSVvvmk"
      },
      "source": [
        "#### PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "cmzILK_Oa4ee"
      },
      "outputs": [],
      "source": [
        "def resize_to_10x10(img_array):\n",
        "    \"\"\"\n",
        "    Resizes any image to (10,10) while preserving the number of channels.\n",
        "    \"\"\"\n",
        "    if img_array.shape[0:2] != (10,10):\n",
        "        print(f\"Resizing image from {img_array.shape[0:2]} to (10, 10)\")\n",
        "        channels = img_array.shape[2]\n",
        "        resized_bands = []\n",
        "        for c in range(channels):\n",
        "            resized_band = resize(img_array[..., c], (10, 10), mode='constant', anti_aliasing=True)\n",
        "            resized_bands.append(resized_band)\n",
        "        img_array = np.stack(resized_bands, axis=-1)\n",
        "    return img_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "ORuJtxAcufhR"
      },
      "outputs": [],
      "source": [
        "def reshape_to_images_using_img_pos(pca_df, selected_pcs):\n",
        "    \"\"\"\n",
        "    Converts PCA-transformed pixel-level data back into images using 'img_pos' and 'Shape' metadata.\n",
        "    Returns images as a list, plus their labels and sample numbers.\n",
        "    This is flexible for images of varying shapes.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    sample_nums = []\n",
        "\n",
        "    grouped = pca_df.groupby(['Sample_num', 'Shape'])\n",
        "    for (sample_num, shape_str), group in grouped:\n",
        "        shape_str_clean = shape_str.replace('(', '').replace(')', '')\n",
        "        h, w = map(int, shape_str_clean.split(','))\n",
        "\n",
        "        img_array = np.full((h, w, len(selected_pcs)), np.nan, dtype=np.float32)\n",
        "        group = group.copy()\n",
        "        group['row_col'] = group['img_pos'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        for _, row in group.iterrows():\n",
        "            row_, col_ = row['row_col']\n",
        "            if 0 <= row_ < h and 0 <= col_ < w:\n",
        "                for c_idx, pc in enumerate(selected_pcs):\n",
        "                    img_array[row_, col_, c_idx] = row[pc]\n",
        "\n",
        "        img_array = np.nan_to_num(img_array, nan=0.0)\n",
        "        label = group['Label_Encoded'].mode()[0]\n",
        "        images.append(img_array)\n",
        "        labels.append(label)\n",
        "        sample_nums.append(sample_num)\n",
        "\n",
        "    print(f\"Created {len(images)} images from PCA components {selected_pcs}.\")\n",
        "    return images, np.array(labels), np.array(sample_nums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "6kLuV41fBYoV"
      },
      "outputs": [],
      "source": [
        "def remove_infrequent_image_labels(images, labels, sample_nums, label_encoder, min_frequency=50):\n",
        "    \"\"\"\n",
        "    Removes classes that have fewer than `min_frequency` images at the image level.\n",
        "    Ensures adequate representation for training.\n",
        "    \"\"\"\n",
        "    label_counts = Counter(labels)\n",
        "    valid_labels = {lbl for lbl, cnt in label_counts.items() if cnt >= min_frequency}\n",
        "    kept_classes = label_encoder.inverse_transform(list(valid_labels)) if valid_labels else []\n",
        "    print(f\"Classes retained (≥ {min_frequency} images): {kept_classes}\")\n",
        "\n",
        "    if not valid_labels:\n",
        "        raise ValueError(\"No classes meet the minimum frequency requirement.\")\n",
        "\n",
        "    mask = np.array([lbl in valid_labels for lbl in labels])\n",
        "    images_filtered = [images[i] for i in range(len(images)) if mask[i]]\n",
        "    labels_filtered = labels[mask]\n",
        "    sample_nums_filtered = sample_nums[mask]\n",
        "\n",
        "    print(f\"Original image count: {len(images)}\")\n",
        "    print(f\"Filtered image count: {len(images_filtered)}\")\n",
        "\n",
        "    return images_filtered, labels_filtered, sample_nums_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYzc_AOD-Ors"
      },
      "source": [
        "#### Vizualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "ruVAsIgS-SFt"
      },
      "outputs": [],
      "source": [
        "def plot_pca_variance(explained_variance_ratio, selected_n_components, threshold_line=0.95):\n",
        "    \"\"\"\n",
        "    Plots explained variance per component and the cumulative explained variance.\n",
        "\n",
        "    Parameters:\n",
        "    - explained_variance_ratio (array): Array of explained variance ratios for each PCA component.\n",
        "    - selected_n_components (int): The number of components selected based on criteria.\n",
        "    - threshold_line (float): The variance threshold (e.g., 0.95) to highlight on the plot.\n",
        "    \"\"\"\n",
        "    cum_variance = np.cumsum(explained_variance_ratio)\n",
        "    n_components = len(explained_variance_ratio)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    # Plot explained variance ratio\n",
        "    ax1.bar(range(1, n_components + 1), explained_variance_ratio, alpha=0.7, label='Explained Variance per Component')\n",
        "    ax1.set_xlabel('Principal Component')\n",
        "    ax1.set_ylabel('Explained Variance Ratio', color='b')\n",
        "    ax1.set_xticks(range(1, n_components + 1))\n",
        "    ax1.tick_params(axis='y', labelcolor='b')\n",
        "\n",
        "    # Plot cumulative variance on a secondary axis\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(range(1, n_components + 1), cum_variance, color='r', marker='o', label='Cumulative Explained Variance')\n",
        "    ax2.axhline(y=threshold_line, color='g', linestyle='--', label=f'{threshold_line*100}% Threshold')\n",
        "    ax2.set_ylabel('Cumulative Explained Variance', color='r')\n",
        "    ax2.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "    # Highlight the selected number of components\n",
        "    ax1.axvline(x=selected_n_components, color='k', linestyle='--', label=f'Selected: {selected_n_components} PCs')\n",
        "\n",
        "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.95))\n",
        "    plt.title('PCA Explained Variance and Cumulative Explained Variance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kxtYGdRvoIR"
      },
      "source": [
        "#### Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "2bShA3IyueYY"
      },
      "outputs": [],
      "source": [
        "def stratified_image_level_split(sample_nums, labels, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Performs a stratified split at the image level (Sample_num).\n",
        "    Ensures no data leakage from the same area into both train and test sets.\n",
        "    \"\"\"\n",
        "    unique_df = pd.DataFrame({'Sample_num': sample_nums, 'Label_Encoded': labels}).drop_duplicates()\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    train_idx, test_idx = next(sss.split(unique_df['Sample_num'], unique_df['Label_Encoded']))\n",
        "    train_samples = unique_df['Sample_num'].iloc[train_idx].values\n",
        "    test_samples = unique_df['Sample_num'].iloc[test_idx].values\n",
        "    return train_samples, test_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "Fkb60rLvuh7_"
      },
      "outputs": [],
      "source": [
        "def split_data_by_samples(images, labels, sample_nums, train_samples, test_samples):\n",
        "    \"\"\"\n",
        "    Splits data into train/test sets based on Sample_num.\n",
        "    Also resizes all images to 10x10 and converts them into a uniform NumPy array.\n",
        "    \"\"\"\n",
        "    train_mask = np.isin(sample_nums, train_samples)\n",
        "    test_mask = np.isin(sample_nums, test_samples)\n",
        "\n",
        "    X_train_list = [images[i] for i in range(len(images)) if train_mask[i]]\n",
        "    y_train = labels[train_mask]\n",
        "    X_test_list = [images[i] for i in range(len(images)) if test_mask[i]]\n",
        "    y_test = labels[test_mask]\n",
        "\n",
        "    print(f\"Training set: {len(X_train_list)} images\")\n",
        "    print(f\"Testing set: {len(X_test_list)} images\")\n",
        "\n",
        "    if len(X_test_list) == 0:\n",
        "        raise ValueError(\"Test set is empty. Adjust your splitting or dataset.\")\n",
        "\n",
        "    # Resize all images to (10,10)\n",
        "    X_train = np.array([resize_to_10x10(img) for img in X_train_list], dtype=np.float32)\n",
        "    X_test = np.array([resize_to_10x10(img) for img in X_test_list], dtype=np.float32)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJm85j0Dvlu0"
      },
      "source": [
        "#### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "0RbMaH4GvlWD"
      },
      "outputs": [],
      "source": [
        "def determine_pca_components(X_train_scaled, variance_threshold=0.9):\n",
        "  \"\"\"\n",
        "  Determine the number of PCA components needed to explain at least `variance_threshold` of the variance.\n",
        "  \"\"\"\n",
        "  pca_full = PCA()\n",
        "  pca_full.fit(X_train_scaled)\n",
        "  cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "  n_components = np.searchsorted(cumulative_variance, variance_threshold) + 1\n",
        "  print(f\"Number of components to reach {variance_threshold*100}% variance: {n_components}\")\n",
        "  return n_components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "64oekXcOqd9L"
      },
      "outputs": [],
      "source": [
        "def apply_pca_pixel_level(train_df, test_df, frequency_cols):\n",
        "    \"\"\"\n",
        "    Apply PCA at pixel level, fit only on training pixels to avoid data leakage, and transform the test set.\n",
        "\n",
        "    Steps:\n",
        "    1. Scale training data and fit PCA with all components (tbd).\n",
        "    2. Analyze  cumulative explained variance and dynamically select components:\n",
        "       - Get cumulative variance.\n",
        "       - Use a 95% baseline threshold.\n",
        "       - If remaining components contributes <2% added variance beyond 95%, limit to ~95%.\n",
        "         Otherwise, keep all that cross  main threshold.\n",
        "    3. Transform both train and test sets using selected components.\n",
        "    4. Return updated DataFrames with selected principal components and fitted PCA object.\n",
        "    \"\"\"\n",
        "    # Extract arrays\n",
        "    X_train = train_df[frequency_cols].values\n",
        "    X_test = test_df[frequency_cols].values\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Fit PCA with all components\n",
        "    pca_full = PCA()\n",
        "    pca_full.fit(X_train_scaled)\n",
        "\n",
        "    explained_variance_ratio = pca_full.explained_variance_ratio_\n",
        "    cum_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "    # Set baseline thresholds\n",
        "    main_threshold = 0.95  # Min desired cumulative variance\n",
        "    additional_variance_cutoff = 0.01  # <1% added variance beyond main threshold\n",
        "\n",
        "    # Find the min components to reach main threshold (95%)\n",
        "    main_components = np.searchsorted(cum_variance, main_threshold) + 1\n",
        "\n",
        "    # Plot variance analysis\n",
        "    plot_pca_variance(explained_variance_ratio, main_components, threshold_line=main_threshold)\n",
        "\n",
        "    # Refit PCA with selected num of components\n",
        "    pca = PCA(n_components=main_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    # Build PC column names\n",
        "    pc_columns = [f'PC{i+1}' for i in range(main_components)]\n",
        "\n",
        "    # Create DataFrames with selected PCs\n",
        "    train_pca_df = pd.concat([train_df.reset_index(drop=True), pd.DataFrame(X_train_pca, columns=pc_columns)], axis=1)\n",
        "    test_pca_df = pd.concat([test_df.reset_index(drop=True), pd.DataFrame(X_test_pca, columns=pc_columns)], axis=1)\n",
        "\n",
        "    print(f\"\\nPCA completed with {main_components} components.\")\n",
        "    print(f\"Total explained variance: {cum_variance[main_components-1]*100:.2f}%\")\n",
        "\n",
        "    return train_pca_df, test_pca_df, pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpXKZ_-6WqSF"
      },
      "source": [
        "#### RGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "pRO2U0hcWsGK"
      },
      "outputs": [],
      "source": [
        "def create_rgb_images(samples_df, rgb_bands):\n",
        "    images = []\n",
        "    labels = []\n",
        "    sample_nums = []\n",
        "\n",
        "    grouped = samples_df.groupby(['Sample_num', 'Shape'])\n",
        "\n",
        "    for (sample_num, shape_str), group in grouped:\n",
        "        shape_str_clean = shape_str.replace('(', '').replace(')', '')\n",
        "        h, w = map(int, shape_str_clean.split(','))\n",
        "\n",
        "        img_array = np.full((h, w, 3), np.nan, dtype=np.float32)\n",
        "        group = group.copy()\n",
        "        group['row_col'] = group['img_pos'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        for i, row in group.iterrows():\n",
        "            row_, col_ = row['row_col']\n",
        "            if 0 <= row_ < h and 0 <= col_ < w:\n",
        "                img_array[row_, col_, 0] = row[rgb_bands[0]]\n",
        "                img_array[row_, col_, 1] = row[rgb_bands[1]]\n",
        "                img_array[row_, col_, 2] = row[rgb_bands[2]]\n",
        "\n",
        "        img_array = np.nan_to_num(img_array, nan=0.0)\n",
        "        # No resizing yet\n",
        "\n",
        "        label = group['Label_Encoded'].mode()[0]\n",
        "        images.append(img_array)\n",
        "        labels.append(label)\n",
        "        sample_nums.append(sample_num)\n",
        "\n",
        "    print(f\"Created {len(images)} RGB images using bands {rgb_bands}.\")\n",
        "    return images, np.array(labels), np.array(sample_nums)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1VeG_fuIAN"
      },
      "source": [
        "#### Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "bnocVXvBAYGd"
      },
      "outputs": [],
      "source": [
        "def get_micro_cnn(input_shape, num_classes, learning_rate=1e-4, drop_rate=0.2):\n",
        "  model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    # Output Layer\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "DhxnA3wnFDsL"
      },
      "outputs": [],
      "source": [
        "def get_macro_cnn(input_shape, num_classes, learning_rate=1e-4, drop_rate=.2):\n",
        "  model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Block 2\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(drop_rate),\n",
        "\n",
        "    # Output Layer\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "B3oO3ADluRVp"
      },
      "outputs": [],
      "source": [
        "def get_rgb_cnn(input_shape, num_classes, learning_rate=5e-4):\n",
        "  model = models.Sequential([\n",
        "      layers.Input(shape=input_shape),\n",
        "      layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(128, activation='relu'),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "7cRL5yuauFBL"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_2d_model(model, X_train, y_train_cat, X_test, y_test_cat, model_name, checkpoint_path, epochs=10, batch_size=32):\n",
        "    callbacks = define_callbacks(checkpoint_path)  # Use the 1D define_callbacks directly\n",
        "    history = model.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat),\n",
        "                        epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=1)\n",
        "    print(f\"{model_name} Test Accuracy: {test_acc:.4f}\")\n",
        "    return model, history, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0X9PZf_zuuV"
      },
      "source": [
        "### Execute Pipeline\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z3cxaHFKvF2J",
        "outputId": "3f7c9635-58d6-4fd4-9614-77d6d09c46a1"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "samples_df = pd.read_csv(samples_path)\n",
        "\n",
        "# Combine classes\n",
        "samples_df = combine_classes(samples_df, ['Natural Grassland', 'Shrubs'], 'Shrubs and Natural Grassland')\n",
        "\n",
        "# Remove wetlands\n",
        "samples_df = remove_wetlands(samples_df)\n",
        "\n",
        "# Remove planted forest\n",
        "# samples_df = remove_planted_forest(samples_df)\n",
        "\n",
        "# Preprocess data (remove NaNs, -9999, label encode)\n",
        "samples_df, label_encoder = preprocess_data(samples_df)\n",
        "\n",
        "# Get the Freq. cols\n",
        "frequency_cols = [c for c in samples_df.columns if c.startswith('frq')]\n",
        "\n",
        "# Set number of classes\n",
        "num_classes=len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "chpKMsRCA4PQ",
        "outputId": "42b41baa-73f8-49d5-e128-050e0a3cf711"
      },
      "outputs": [],
      "source": [
        "# Show initial class distribution at pixel-level\n",
        "plot_class_distribution(samples_df['Label'].values, label_encoder.classes_, \"Initial Class Distribution (Pixel-level)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjOK5zuPKXR6"
      },
      "source": [
        "#### Stratified Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "0s1PrXL8q6RU"
      },
      "outputs": [],
      "source": [
        "# Image-level splitting before PCA to avoid data leakage\n",
        "# We have pixel-level data with Sample_num and Label_Encoded.\n",
        "unique_samples = samples_df[['Sample_num', 'Label_Encoded']].drop_duplicates()\n",
        "train_samples, test_samples = stratified_image_level_split(unique_samples['Sample_num'].values,\n",
        "                                                           unique_samples['Label_Encoded'].values,\n",
        "                                                           test_size=0.2)\n",
        "\n",
        "# Ridding warnings\n",
        "train_df = samples_df[samples_df['Sample_num'].isin(train_samples)].copy()\n",
        "test_df = samples_df[samples_df['Sample_num'].isin(test_samples)].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSPum7yCKBXT"
      },
      "source": [
        "#### Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-WbtvfJ9zy",
        "outputId": "c9061ec2-6784-4209-e366-0f2b3ecd66e8"
      },
      "outputs": [],
      "source": [
        "# These are taken from above as well as verifying from actually looking them up\n",
        "# NOTE: I am not oversampling Built-up or Waterbodies cause they are both really easy to identify shown by stats\n",
        "oversample_images = {\n",
        "    'Consolidated Barren': [2051, 21902, 2067, 9620, 2070, 10781, 7080, 10790, 20392, 7081, 7082, 7083, 7084, 7085, 9390, 7983, 9391, 22192, 7092, 7093, 7094, 9398, 7100, 7102, 7103, 20417, 7106, 8005, 21061, 2375, 22982, 24262, 4556, 8018, 8404, 8405, 22872, 4698, 9574, 9576, 9066, 8304, 8305, 8306, 7416, 8185, 8186, 8187, 5374, 4859],\n",
        "    # 'Natural Grassland': [7424, 5634, 27138, 7947, 7692, 25228, 7701, 3991, 6009, 7708, 7711, 2848, 6052, 11812, 5414, 9510, 6952, 24997, 6827, 5936, 6192, 7984, 10801, 6973, 7742, 5063, 8520, 5065, 6602, 7881, 7118, 11087, 4822, 5594, 11103, 9442, 7011, 7652, 6757, 7013, 7656, 3435, 3437, 3565, 26095, 26353, 8825, 28026, 4731, 5629],\n",
        "    'Unconsolidated Barren': [1, 2, 10883, 20610, 21639, 11, 20758, 26, 11682, 11683, 11684, 11685, 21671, 1065, 8235, 20013, 7342, 26416, 20026, 8507, 20034, 20676, 8141, 213, 20056, 10714, 10715, 10716, 10717, 10718, 10719, 10720, 10721, 10722, 10723, 10724, 10725, 10726, 10727, 20839, 27751, 20591, 11633, 11634, 11635, 7412, 20084, 20087, 22234, 20090],\n",
        "    'Planted Forest': [7303, 5899, 5771, 8460, 5389, 5396, 5653, 5403, 5919, 8226, 8227, 2853, 5931, 5421, 7982, 5422, 6318, 5684, 5686, 5687, 5944, 5945, 6328, 6331, 6460, 5949, 5950, 6334, 5579, 5707, 5710, 5711, 6352, 5713, 5586, 5587, 5968, 5464, 5977, 5473, 5477, 5481, 5611, 5613, 5487, 5999, 6000, 6386, 6387, 6005],\n",
        "    'Permanent Crops': [24067, 27268, 22533, 2829, 3983, 23444, 24084, 21782, 26263, 24856, 27163, 6044, 27547, 21022, 6046, 25249, 23715, 5028, 23849, 23724, 8495, 8496, 8497, 5042, 24115, 5043, 23608, 25789, 23623, 21964, 27597, 23252, 8406, 8407, 21209, 21983, 26336, 21985, 24930, 24041, 20971, 4855, 24178, 27251, 24950, 26871, 27643, 3708, 23166, 5887],\n",
        "    'Annual Crops': [1538, 3331, 4, 3205, 2950, 7, 3078, 4739, 266, 4998, 1429, 8471, 3992, 7835, 2204, 5408, 2980, 4009, 3120, 1585, 3248, 1588, 2872, 5563, 3516, 1470, 8386, 4931, 1092, 1604, 2885, 10438, 1224, 5453, 8527, 1619, 3795, 3798, 1629, 3294, 2402, 1507, 3044, 2407, 4840, 1645, 3187, 4854, 3831, 3833],\n",
        "}\n",
        "# Modifies train_df in-place returns new df.\n",
        "train_df = oversample_to_target_size(train_df, oversample_images, percentage=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "PGAJk4raKgI4"
      },
      "outputs": [],
      "source": [
        "# Define target_size_per_class for augmentation\n",
        "# Get largest class size post-oversampling (Obviously shrubs lol)\n",
        "largest_class_size = (train_df['Label'] == 'Shrubs and Natural Grassland').sum()\n",
        "target_size_per_class = {\n",
        "    class_label: max(\n",
        "        len(train_df[train_df['Label'] == class_label]),\n",
        "        int(largest_class_size * (3 / 100.0))\n",
        "    )\n",
        "    for class_label in oversample_images.keys()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7emlASQbLj5x"
      },
      "source": [
        "#### Apply Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeXV3b4wLoDy",
        "outputId": "b036ec51-c1d4-455c-ab90-fa4aa1a19f40"
      },
      "outputs": [],
      "source": [
        "train_df = augment_data(\n",
        "    train_df,\n",
        "    oversample_images,\n",
        "    target_size_per_class,\n",
        "    frequency_cols\n",
        ")\n",
        "\n",
        "# Shuffle the augmented training data\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEAnIFJPKDUi"
      },
      "source": [
        "#### Apply PCA & reconstruct Images from PCA DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "collapsed": true,
        "id": "rCB2ZdUNEZUM",
        "outputId": "7c39e8ac-7dbd-4967-8ca6-37e1896ed004"
      },
      "outputs": [],
      "source": [
        "# Apply PCA on pixel-level data (training only), then transform test\n",
        "train_pca_df, test_pca_df, pca = apply_pca_pixel_level(train_df, test_df, frequency_cols)\n",
        "\n",
        "# Determine available PCs\n",
        "available_pcs = [c for c in train_pca_df.columns if c.startswith('PC')]\n",
        "\n",
        "# Using ALL selected_pcs = available_pcs (Jsut trying it out...)\n",
        "selected_pcs = available_pcs\n",
        "print(\"Selected Principal Components:\", selected_pcs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM8bUMP2rjF-",
        "outputId": "345fef65-45b9-413f-8451-8b48fc78a857"
      },
      "outputs": [],
      "source": [
        "# Reconstruct images from PCA DataFrames\n",
        "X_train_pca_images, y_train_labels, train_image_sample_nums = reshape_to_images_using_img_pos(train_pca_df, selected_pcs=selected_pcs)\n",
        "X_test_pca_images, y_test_labels, test_image_sample_nums = reshape_to_images_using_img_pos(test_pca_df, selected_pcs=selected_pcs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13npzGsyKGhz"
      },
      "source": [
        "#### Filter categories with < 50 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "ernNrA2O1DeS"
      },
      "outputs": [],
      "source": [
        "# Combine train and test sets before filtering infrequent classes\n",
        "X_all_pca_images = X_train_pca_images + X_test_pca_images\n",
        "y_all_labels = np.concatenate([y_train_labels, y_test_labels])\n",
        "all_image_sample_nums = np.concatenate([train_image_sample_nums, test_image_sample_nums])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNTAkBVTrlD0",
        "outputId": "13957705-ba6f-4a1b-aca6-1359355634b6"
      },
      "outputs": [],
      "source": [
        "# Remove infrequent classes at the image level\n",
        "X_all_pca_images, y_all_labels, all_image_sample_nums = remove_infrequent_image_labels(\n",
        "    X_all_pca_images, y_all_labels, all_image_sample_nums, label_encoder, min_frequency=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Bkhhb40grlc8",
        "outputId": "fee8baf2-b7e6-4146-ec34-cf947652e0fd"
      },
      "outputs": [],
      "source": [
        "# Check distribution after removal\n",
        "plot_class_distribution(y_all_labels, label_encoder.classes_, \"All Set (Image-Level) After Removal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sFNenPSKN0_"
      },
      "source": [
        "#### Final Split at Image Level and Resize to (10,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nLbprfrNrlfU",
        "outputId": "d812108f-117c-4f27-c0b2-27dae860c2ba"
      },
      "outputs": [],
      "source": [
        "# Now split & resize data into train/test sets, ensuring distinct Sample_nums\n",
        "X_train, X_test, y_train, y_test = split_data_by_samples(\n",
        "    X_all_pca_images, y_all_labels, all_image_sample_nums,\n",
        "    train_samples, test_samples\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oUhrcXZrlha",
        "outputId": "f874d72e-c5b5-4a92-a2ea-50c5a08df643"
      },
      "outputs": [],
      "source": [
        "# Verify shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "assert X_train.shape[1:] == (10, 10, len(selected_pcs)), f\"Invalid X_train shape: {X_train.shape}\"\n",
        "if X_test.shape[0] > 0:\n",
        "    assert X_test.shape[1:] == (10, 10, len(selected_pcs)), f\"Invalid X_test shape: {X_test.shape}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "c4rugBFXzn9M"
      },
      "outputs": [],
      "source": [
        "# Convert labels to categorical\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB9f1K6JMrUh"
      },
      "source": [
        "#### Get Class Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cK-al6-MssE",
        "outputId": "ace81166-35f4-49e4-b53e-32d6e2b63dcc"
      },
      "outputs": [],
      "source": [
        "class_weights = get_class_weights(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34vBq0rRBxMs"
      },
      "source": [
        "### Micro CNN\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "-nt91wCdM7Hf"
      },
      "outputs": [],
      "source": [
        "# Get the CNN model and Train\n",
        "micro_model = get_micro_cnn(X_train.shape[1:], num_classes, drop_rate=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "3HrDu7ZZSoKT",
        "outputId": "d646f48b-2721-49b5-9dfe-996dca939e30"
      },
      "outputs": [],
      "source": [
        "micro_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXFUYm2yM9w7"
      },
      "source": [
        "#### Train without weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R_iJxfyEwPKQ",
        "outputId": "63759b9c-1ab4-4bb9-f18b-ffe53c1eba48"
      },
      "outputs": [],
      "source": [
        "micro_history = train_model(micro_model, X_train, y_train_cat, X_test, y_test_cat,\n",
        "                            epochs=100, batch_size=32, checkpoint_path=microcnn_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPU4SK1vM11Z"
      },
      "source": [
        "#### Train Using Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw90TFVtM0-H"
      },
      "outputs": [],
      "source": [
        "# micro_history = train_model_with_class_weights(\n",
        "#     micro_model,\n",
        "#     X_train,\n",
        "#     y_train_cat,\n",
        "#     X_test,\n",
        "#     y_test_cat,\n",
        "#     class_weights=class_weights,\n",
        "#     epochs=100,\n",
        "#     batch_size=32\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXp2C0PpCo22",
        "outputId": "b447280d-dca9-4bc8-ce2f-2700468113b3"
      },
      "outputs": [],
      "source": [
        "# Get the testing accuracy\n",
        "micro_test_acc = evaluate_model(micro_model, X_test, y_test_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "collapsed": true,
        "id": "Nu6l8nItCrIE",
        "outputId": "4fd9bb7d-c21e-4c5a-c7c1-1a42c79e6c22"
      },
      "outputs": [],
      "source": [
        "# Plotting the training curves\n",
        "plot_training_curves(micro_history, \"MicroCNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3sqgOTrQCtYd",
        "outputId": "65d11de1-caed-4e1a-b1dd-dc30e9cf7ac7"
      },
      "outputs": [],
      "source": [
        "# Get the predictions and create a CM\n",
        "y_true_labels, y_pred_labels, _, _ = get_predictions(micro_model, X_test, y_test_cat, label_encoder)\n",
        "plot_confusion_matrix(y_true_labels, y_pred_labels, label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C1pqt3jCuW2",
        "outputId": "09dbaf91-4817-46ea-801c-3f7a8adbcb61"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report (MicroCNN):\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdCum_nEBxMt"
      },
      "source": [
        "### MacroCNN\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D_EMfkSzwUHj",
        "outputId": "4a3ea067-c023-49c8-841f-b0832a2009a7"
      },
      "outputs": [],
      "source": [
        "# Train MacroCNN\n",
        "macro_model = get_macro_cnn(X_train.shape[1:], len(label_encoder.classes_))\n",
        "macro_history = train_model(macro_model, X_train, y_train_cat, X_test, y_test_cat, epochs=50, batch_size=32, checkpoint_path=macrocnn_checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcz22y-TCwXA",
        "outputId": "6dc25832-3e86-4b2f-abdd-14ef65cf53e5"
      },
      "outputs": [],
      "source": [
        "macro_test_acc = evaluate_model(macro_model, X_test, y_test_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "tCUtd0x-CyN_",
        "outputId": "df2ab995-2978-4bc0-b09f-72b66bae3326"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(macro_history, \"MacroCNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "36H1IEsZC0iS",
        "outputId": "f44cc5db-c870-4d62-eaf3-1a3d80fec1e7"
      },
      "outputs": [],
      "source": [
        "y_true_labels_macro, y_pred_labels_macro, _, _ = get_predictions(macro_model, X_test, y_test_cat, label_encoder)\n",
        "plot_confusion_matrix(y_true_labels_macro, y_pred_labels_macro, label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KOWZJxlC1lL",
        "outputId": "3bf9f828-1c57-4db8-9de3-c6dc93749718"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report (MacroCNN):\")\n",
        "print(classification_report(y_true_labels_macro, y_pred_labels_macro, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WmFvh-RBxMu"
      },
      "source": [
        "### RGB CNN\n",
        "\n",
        "RGB CNN (No PCA, uses RGB-like bands)\n",
        " - Idea is that data is an array of shape (num_images,10,10,3)\n",
        " extracted from three specific frequency bands chosen.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gH856oq676qa",
        "outputId": "8883324c-82ab-459a-b79c-caa71c814ec3"
      },
      "outputs": [],
      "source": [
        "### RGB CNN (No PCA, uses RGB-like bands)\n",
        "# Extracted from three specific frequency bands: 'frq21', 'frq35', 'frq57'\n",
        "\n",
        "# Step 1: Create RGB Images\n",
        "rgb_bands = ['frq21', 'frq35', 'frq57']\n",
        "rgb_images, rgb_labels, rgb_sample_nums = create_rgb_images(samples_df, rgb_bands)\n",
        "\n",
        "# Step 2: Show class distribution at image-level before removal\n",
        "plot_class_distribution(rgb_labels, label_encoder.classes_, \"Class Distribution (RGB images) Before Removal\")\n",
        "\n",
        "# Step 3: Combine Train and Test Samples Before Filtering Infrequent Classes\n",
        "# Ensure uniform filtering process across train and test sets\n",
        "rgb_images_combined, rgb_labels_combined, rgb_sample_nums_combined = remove_infrequent_image_labels(\n",
        "    rgb_images, rgb_labels, rgb_sample_nums, label_encoder, min_frequency=50\n",
        ")\n",
        "\n",
        "# Step 4: Stratified Split After Filtering\n",
        "train_samples_rgb, test_samples_rgb = stratified_image_level_split(\n",
        "    rgb_sample_nums_combined, rgb_labels_combined, test_size=0.2\n",
        ")\n",
        "\n",
        "# Step 5: Split into Train/Test Sets and Resize Images\n",
        "X_train_rgb, X_test_rgb, y_train_rgb, y_test_rgb = split_data_by_samples(\n",
        "    rgb_images_combined, rgb_labels_combined, rgb_sample_nums_combined,\n",
        "    train_samples_rgb, test_samples_rgb\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gf9xGOZU77KK",
        "outputId": "8ebfaf7b-ef9a-4a21-e0a6-286f71d4fa9c"
      },
      "outputs": [],
      "source": [
        "# Step 6: Verify Shapes and Resize Check\n",
        "assert X_train_rgb.shape[1:] == (10, 10, 3), f\"Invalid X_train shape: {X_train_rgb.shape}\"\n",
        "assert X_test_rgb.shape[1:] == (10, 10, 3), f\"Invalid X_test shape: {X_test_rgb.shape}\"\n",
        "\n",
        "print(\"X_train_rgb shape:\", X_train_rgb.shape)\n",
        "print(\"X_test_rgb shape:\", X_test_rgb.shape)\n",
        "\n",
        "# Step 7: Show Class Distribution in Train/Test Sets\n",
        "plot_class_distribution(y_train_rgb, label_encoder.classes_, \"RGB Train Set Class Distribution\")\n",
        "plot_class_distribution(y_test_rgb, label_encoder.classes_, \"RGB Test Set Class Distribution\")\n",
        "\n",
        "# Step 8: Convert Labels to Categorical\n",
        "y_train_rgb_cat = to_categorical(y_train_rgb, num_classes=len(label_encoder.classes_))\n",
        "y_test_rgb_cat = to_categorical(y_test_rgb, num_classes=len(label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWb2QRvC7840",
        "outputId": "622d5728-c5bd-4ede-f35b-e29d800408b1"
      },
      "outputs": [],
      "source": [
        "# Step 10: Train and Evaluate the RGB CNN\n",
        "# Step 9: Get the RGB CNN Model\n",
        "rgb_model = get_rgb_cnn(X_train_rgb.shape[1:], len(label_encoder.classes_))\n",
        "\n",
        "rgb_history = train_model(rgb_model, X_train_rgb, y_train_rgb_cat, X_test_rgb, y_test_rgb_cat, epochs=50,\n",
        "                            batch_size=32, checkpoint_path=rgbcnn_checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "3CS39jI0787E",
        "outputId": "74620de5-4246-4317-8676-66d56baba047"
      },
      "outputs": [],
      "source": [
        "# Step 11: Evaluate Model Accuracy\n",
        "rgb_test_accuracy = evaluate_model(rgb_model, X_test_rgb, y_test_rgb_cat)\n",
        "print(f\"RGB CNN Test Accuracy: {rgb_test_accuracy:.4f}\")\n",
        "\n",
        "# Step 12: Plot Training Curves\n",
        "plot_training_curves(rgb_history, \"RGB CNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vmFLSWKb789B",
        "outputId": "d30f24dd-488e-4482-ed03-595686c1547a"
      },
      "outputs": [],
      "source": [
        "# Step 13: Generate Predictions and Confusion Matrix\n",
        "y_true_labels_rgb, y_pred_labels_rgb, _, _ = get_predictions(\n",
        "    rgb_model, X_test_rgb, y_test_rgb_cat, label_encoder\n",
        ")\n",
        "plot_confusion_matrix(y_true_labels_rgb, y_pred_labels_rgb, label_encoder)\n",
        "\n",
        "# Step 14: Print Classification Report\n",
        "print(\"Classification Report (RGB CNN):\")\n",
        "print(classification_report(y_true_labels_rgb, y_pred_labels_rgb, target_names=label_encoder.classes_))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PrRTgF60ysw4",
        "S7KyCQKzsROi",
        "WpyEF9RsE5Zq",
        "sD4t_cmHEU_R",
        "ivk1jcwZDrUV",
        "4rGd7SwwLJzG",
        "_nZuYwmhxzCw",
        "IZ7NlyxbICoz",
        "UlKQP_x7Qmh_",
        "Ey36DfFIzSOi",
        "KGf_Vwn4tDeT",
        "S-1KIjFCR9rt",
        "YGBJzTbJmsg1",
        "yARuJ0qzrN2y",
        "TlMgwHFF6D_s",
        "RiheAiQysCSJ",
        "PcokXGwHsCSL",
        "TkFL7_8YsCSL",
        "EERyEFk_sCSM",
        "2P-wHZ_yrtkl",
        "B0ehJfuDrtkm",
        "sXmeVHSVvvmk",
        "FYzc_AOD-Ors",
        "2kxtYGdRvoIR",
        "eJm85j0Dvlu0",
        "UpXKZ_-6WqSF",
        "G0X9PZf_zuuV",
        "yjOK5zuPKXR6",
        "PSPum7yCKBXT",
        "7emlASQbLj5x",
        "xEAnIFJPKDUi",
        "13npzGsyKGhz",
        "FB9f1K6JMrUh",
        "yXFUYm2yM9w7",
        "HPU4SK1vM11Z",
        "OdCum_nEBxMt",
        "7WmFvh-RBxMu"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
